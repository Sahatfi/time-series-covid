{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1380ec3-68d6-4251-b33f-cfafe5e201f2",
   "metadata": {},
   "source": [
    "# 1 Introduction and clinical context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff6cdd6-ece6-4280-92a2-b8e7018e743b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "837603f9-98c3-43cc-a00a-10768720e757",
   "metadata": {},
   "source": [
    "# 2 Defintions methods and hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab27541-893b-43c3-bcf3-db97016290c4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2813cd73-9773-48ee-bf5b-c6f432724636",
   "metadata": {},
   "source": [
    "# 3 Import of data and libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a6f835-69c6-4839-888a-cac11becc2ee",
   "metadata": {},
   "source": [
    "## 3.1 Import of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc793ca8-df34-4fa3-9f93-eb5e84594b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing basic data analyst libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Importing libraries for SQL querrying\n",
    "import duckdb "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d3603c-6982-4308-82b7-f144456b5a09",
   "metadata": {},
   "source": [
    "## 3.2 Import of tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d98b1a6-2877-4596-8313-9e1867f18c41",
   "metadata": {},
   "source": [
    "I decided to import all the tables relevant for my analysis at this point. I will treat them as a database and later on i will look at them in detail. Now I will just import them one by one whereas querrying and cleaning comes a bit later in separate stages. I chose this approach because i find it more logically organised compared to other solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd34dc1-8988-482c-80ff-78f6dc9c6914",
   "metadata": {},
   "source": [
    "###3.2.1 Importing confirmed cases table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661943cf-0e35-4eed-a45c-1802dd7ad2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases = pd.read_csv('data/raw/confirmed_global.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389d255f-b2b1-47f5-a01d-dc9f4d52a686",
   "metadata": {},
   "source": [
    "Verifying import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7979b4f-3849-4ae5-8b43-54d506e81b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cases.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba95d563-b0fe-4301-9f97-e7ef7ce91214",
   "metadata": {},
   "source": [
    "##### ---Cases dataset imported successfully---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7406c85-43d8-474d-b076-1f6a0d66bf54",
   "metadata": {},
   "source": [
    "### 3.2.2 Importing death cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e9a1e5-fbde-4846-86c4-74ad856c91f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deaths = pd.read_csv('data/raw/deaths_global.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee17518-fdf9-415f-a225-30aa54526cdc",
   "metadata": {},
   "source": [
    "Verifying import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9ebbd4-552f-4eae-bf0a-3723c94e98c4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(df_deaths.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cff8e2-9fd7-4a8b-8031-061c217c12cf",
   "metadata": {},
   "source": [
    "##### ----Deaths dataset importet successfully----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d486258-b68b-4077-b375-c708b51cdeef",
   "metadata": {},
   "source": [
    "### 3.2.3 Importing recovered cases dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7429274-3c2c-4410-aba5-01614f548271",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recovered = pd.read_csv('data/raw/recovered_global.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268d36ef-0074-4bf4-b53f-e40199f9ae18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_recovered.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318a75a5-b315-47f5-b3d6-883e27c35c2d",
   "metadata": {},
   "source": [
    "##### ----Recovered dataset importet successfully!----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e578c79-44d4-4e1d-a70d-b81e4e0f90c7",
   "metadata": {},
   "source": [
    "# 4 Inspecting cleaning and preparing datasets for melting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709aab47-3590-4e05-98ec-f4ed0969c869",
   "metadata": {},
   "source": [
    "In this sections I will get acquianted with dataset, melt it, deal with missings and prepare it for melting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9734eb-32f6-4d84-a8ca-b753ebb5e81b",
   "metadata": {},
   "source": [
    "## 4.1 Inspecting cleaning and preparing the dataset with cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498f54a9-9221-4a9b-82a5-519f3917cc72",
   "metadata": {},
   "source": [
    "### 4.1.1 Inspecting the dataset with cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7182554c-3b5d-48f9-827a-0bd3264a4e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cases.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b2d98c-5c61-462b-a0c4-2f212d9ae31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cases.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275cbe7b-52aa-44ce-9bb8-1f1e1be53508",
   "metadata": {},
   "source": [
    "Shape of the dataset is 274 rows and 449 columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646d4aa3-da1f-42d7-8be5-dc54897184d0",
   "metadata": {},
   "source": [
    "From the first glance I see identifier columns such as province/state, Country/Relogion, Latitude and longitude. There are 449 columns where majority corresponds to individual dates within given period of time. To confirm that only date columns follow 'Long', and that there are no unexpected columns in between, I will iterate through the column names and count the columns to crosscheck whether i itterate through columns correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81382358-5877-4456-aef9-181a342ba34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for col in df_cases:\n",
    "    print(col)\n",
    "    count = count + 1\n",
    "    \n",
    "print(\"the column count is: \", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3fc440-9597-420c-b431-05b0bd542932",
   "metadata": {},
   "source": [
    "All the respective columns  after 'Long' seem to represent individual dates and column count from the loop is in agreement with the column count from df_cases.head(). Next I need to check data types of individual columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab53c401-eb67-4b49-b574-605532f13000",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_integer = 0\n",
    "count_other_datatypes = 0\n",
    "for col in df_cases:\n",
    "    print(df_cases[col].dtype)\n",
    "    if df_cases[col].dtype == int:\n",
    "        count_integer = count_integer +1\n",
    "    else:\n",
    "        count_other_datatypes = count_other_datatypes + 1\n",
    "\n",
    "print('Integer count is:', count_integer, '\\n Other data types count is:', count_other_datatypes)\n",
    "if count_integer == 445:\n",
    "    print('All date-columns are integers!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a4c21b-f856-4945-aacb-2991efef5148",
   "metadata": {},
   "source": [
    "Loop confirmed that all date columns are formated as an integer datatype. I see that other datatypes in the dataset are objects and float. I will confirm it with command .info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac9d221-0a57-4ca4-ab30-a45762262f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16472b40-830f-46e9-8947-9750ff32ca4c",
   "metadata": {},
   "source": [
    "Verification showed exactly those datatypes i noticed  above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcdc72e-35bb-4f54-8419-97ce6fe2b866",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cases.describe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e5b01c-f0a5-4cd6-a6ed-87f047fe428d",
   "metadata": {},
   "source": [
    "Describe command gives me an overview over the dataset. There are a lot of missing values in province/state. I have to investigate this variable closer. Country/region column will likely be the variable of interest and will serve us as identifier, Whereas latitute and longitude are not interesting forour analysis. Even though some viruses are known to spread better in colder or warmer climates, Cross-country comparisons in cases between the countries are not feasible because we donÂ´t have data on people tested and countries were very different in test measures and  test equipment. In this dataset we lack the data for it. I will not check dataset with .describe(). Quick overview shows that there were 0 cases in a begining whereas number of cases began to increase and fluctuate over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b5aba2-4264-4aaa-a36c-31ace31f99e3",
   "metadata": {},
   "source": [
    "### 4.1.2 Cleaning the dataset with cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8056fb-3ede-41c7-b50c-3244089b1b18",
   "metadata": {},
   "source": [
    "I am checking if there are any missing values in columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83fdd91-f8ca-40e7-91c3-f99ef7eb70d5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_cases.isna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6721e66d-92a9-41e2-860d-5bc2d7065653",
   "metadata": {},
   "source": [
    "I cannot see the whole dataset, but the output indicates that Province/state has missing values. I am going to investigate it further along with other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5686179-6111-47bb-b765-4f29b6944d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcf170c-f367-41c4-9762-3d2cb4c5c6aa",
   "metadata": {},
   "source": [
    "It seems that longitude latitude and province contain misssing values whereas dates seem to contain no missing value. I need to check my assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b9bf89-fed4-431e-b389-73af1e266a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases.isna().any().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988ec2d1-e70e-4ca3-a4ce-e87b87598d98",
   "metadata": {},
   "source": [
    "There are three columns in the df_cases. They are Province/State, Lat, Long."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb40f18a-4920-4760-a138-3b65e5fbeafb",
   "metadata": {},
   "source": [
    "There are three columns in the df_cases. They are Province/State, Lat, Long.\n",
    "Before I go further with cleaning and droping, want to check which column is the most pertinent one to become identifier column during melting. The two most valiable options are Province/state and Country/Region. I will extend the jupyter notebook output to contain up to 500 rows and columns which should be sufficient size to cover the entire df_cases dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0d720c-c5c0-41a7-92e5-a0cf5420bf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extending output for rows\n",
    "pd.set_option('display.max_rows', 300)\n",
    "#extending output for columns\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa707e22-97f6-4821-8cd9-4d82ae058e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases[['Province/State', 'Country/Region']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9297f0f-3c30-496c-a8ba-731855f6745c",
   "metadata": {},
   "source": [
    "I confirm that column 'Province/State' is of no interest as it shows either overseas territories and provinces/states within a few countries.Contrary, Norway and Slovakia are both within Country/Region column which will become our identifier. Now I will strip the dataset from all the unneccesarry columns and rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a71ed0-e4ea-469e-b3f0-17a3f478896e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases_filtered = \"\"\"\n",
    "SELECT *\n",
    "FROM df_cases\n",
    "WHERE \"Country/Region\" = 'Norway' OR \"Country/Region\" = 'Slovakia';\n",
    "\"\"\"\n",
    "df_cases = duckdb.query(df_cases_filtered).to_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b91bbed-0f7f-48b2-8f48-ed09b649830f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Checking whether i filtered rows correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3c87a4-0424-476d-863f-36769166ffc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cases.iloc[:, 1:])\n",
    "type(df_cases), df_cases.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c70464-1fb2-4beb-88e1-1fedb8007324",
   "metadata": {},
   "source": [
    "I correctly filtered just relevant rows and transformed querried data into dataframe. There are two rows- Slovakia and Norway with number of columns(449) remained unchanged. Now I will proceed with dropping all the unneccessarry columns. namely: \n",
    "'Province/State', \n",
    "'Long', \n",
    "'Lat'\n",
    "\n",
    "As mentioned earlier, variables lattitude and longitude are of  no value in our analysis, whereas province/state is variable indicating a region within larger countries.Countries of interest are slovakia and Norway and we have no addotional data on specific locations within those countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40558ed-5126-42fd-8d1c-31c9013f7cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases.drop(['Province/State', 'Long','Lat',], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0270844-3fbd-459b-892c-3ef612393007",
   "metadata": {},
   "source": [
    "Checking if there are some missing values in the dataset:\n",
    "df_cases_filtered.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a50a7e-56fe-4199-9e1f-31e28f0095e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases.isna().any().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834ded40-89e3-4186-8f2b-41302710ac11",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "I correctly dropped all the unneccessarry rows and dataset is ready to melt. But before I will do it I will prepare datasets with deaths and recovered cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108ab597-2659-4fbb-ae4d-e7815a21a4d5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## 4.2 Inspecting cleaning and preparing the dataset with deaths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f65c8d-5045-4faa-b1c9-d1d6c28c0d6e",
   "metadata": {},
   "source": [
    "### 4.2.1 Inspecting the dataset with deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cc65d2-f510-49f0-ad2f-fdb5b56538cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_deaths.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc26e09-f9bd-4ac9-95d0-6a92340aa920",
   "metadata": {},
   "source": [
    "The deaths dataset has the same number of rows(274) and columns (449) as the cases dataset. its a good indication because the process of cleaning checking and extracting of data will be very simmilar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f619f3-a67e-4ed7-be33-9563ec7e2c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_deaths.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b80890-6985-4736-bf4a-5104ff9e787a",
   "metadata": {},
   "source": [
    "Again, earlier dates indicate zero deaths whereas number of deaths towards the later period is higher and steadily increasing Whereas first five variables: 'Province/State' 'Country/Region 'Lat''Long' seem to be identical through all the datasets. I have to confirm this assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a950dd1-0450-4017-8aa2-dfa6ac45e0ec",
   "metadata": {},
   "source": [
    "I am checking what datatypes exist in the dataset deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bff74f1-632d-4944-a839-639948be60f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deaths.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c47b04b-836b-4259-ae98-3062be5e81f8",
   "metadata": {},
   "source": [
    "df_deaths dataset seem to have identical types  two float columns (lat, long), two object columns (Province/State Country/Region) and 445 integer(date columns). I will verify it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fa9e4b-3cd7-4798-983c-a57b1ddb0a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_deaths['Country/Region'].dtype)\n",
    "print(df_deaths['Country/Region'].dtype)\n",
    "print(df_deaths['Long'].dtype)\n",
    "print(df_deaths['Lat'].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d26efbc-8ab9-4cd3-bfb8-a37072901422",
   "metadata": {},
   "source": [
    "I confirm my assumption about float and object columns, I will check the date(integer columns as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf00249-3ea6-43d2-bd3b-9848d3d11140",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering dataset so there are only dates\n",
    "df_deaths_only_date = df_deaths.iloc[:, 4:]\n",
    "#Creating loop to check which type the column is\n",
    "date_counter = 0\n",
    "for col in df_deaths_only_date:\n",
    "    print(df_deaths_only_date[col].dtype)\n",
    "    date_counter +=1 \n",
    "\n",
    "print(date_counter)\n",
    "print(df_deaths_only_date.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19438b9-05fb-4102-afad-646bfffdb84f",
   "metadata": {},
   "source": [
    "I confirm that the dataset contains 445 integers! I am finished with scrutinizing the dataset with deaths and I move further to clean it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2521ef72-685f-4390-af56-08e5e2407305",
   "metadata": {},
   "source": [
    "### 4.2.2 Cleaning the dataset with deaths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d514350-45d0-43a6-903a-b43ffa58c2c5",
   "metadata": {},
   "source": [
    "I first check whether columns contain missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab6d369-fa61-4afa-8557-38162be741d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_deaths.isna().any())\n",
    "df_deaths.isna().any().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4171c175-d511-4661-94eb-1a382c37ff82",
   "metadata": {},
   "source": [
    "#I will check how many columns with missing values are present. if there are three we know they are those:\n",
    " Province/State      \n",
    " Lat       \n",
    " Long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bef2716-f5c8-43ee-974c-738b82d93ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deaths.isna().any().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f261c641-4976-4326-abda-92e1a7161f15",
   "metadata": {},
   "source": [
    "I confirm the assumption about the missing values. Now I check identifier columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9116861f-4988-449b-9edc-04b9acbc2910",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_deaths['Province/State'],df_deaths['Country/Region'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4956901-9207-47d8-8517-385d7c261344",
   "metadata": {},
   "source": [
    "Based on the output, I will drop the Long, Lat and Province/State columns as they are of no interest for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc533768-597b-458a-886d-4619a3f2982c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deaths.drop(['Province/State', 'Long', 'Lat'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d2d708-097e-4ed5-9ca9-08a42f8a0a6e",
   "metadata": {},
   "source": [
    "Filtering rows with Slovakia and Norway and re-writing original dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d276e53-c554-44ba-8eb3-47491acdd964",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deaths.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deb9e7e-54e1-4332-a449-2aa572730c5b",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d4cb18-e926-4a74-b03b-329f48a39b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#row-filtering\n",
    "df_deaths_filtered = \"\"\"\n",
    "SELECT * FROM df_deaths\n",
    "WHERE \"Country/Region\" = 'Slovakia' OR \"Country/Region\" = 'Norway' \"\"\"\n",
    "#re-writing df_deaths\n",
    "df_deaths = duckdb.query(df_deaths_filtered).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adc6966-a51e-4775-9e03-13049508a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(' The dataset has', df_deaths.isna().any().sum(), 'missings', df_deaths.shape, df_deaths.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896cc2f5-188d-4b8a-a9ef-923b6c5627ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_deaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00a9f19-9ee8-4b24-b35b-7f5e049277a9",
   "metadata": {},
   "source": [
    " ##### Dataset is correctly cleaned and filtered with dates as integers, identifier column som object and zero missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95b648b-9443-4874-ba3f-ee59a4ffff45",
   "metadata": {},
   "source": [
    "## 4.3 Inspecting cleaning and preparing the dataset with recovered cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0298750b-1733-484d-8654-2b18a3ee9b71",
   "metadata": {},
   "source": [
    "### 4.3.1 Inspecting the dataset with recovered cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73c0b9b-802f-464a-a407-970573911932",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recovered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05692f8-6997-4665-a8e8-f93082ebe704",
   "metadata": {},
   "source": [
    "The dataset seems to follow the exactly same pattern as the two  datasets above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0346bfcf-270e-4748-abf1-8198cdbfc907",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recovered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e62605-7860-4831-b50d-ab1fbe2c95ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recovered.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286325a8-69d5-4139-8a5c-008e17288817",
   "metadata": {},
   "source": [
    "Contrary to other columns variable lat is coded as an object. It doesnÂ´t have an effect on my decision to drop it, it is just peculiar observation. Now I check whether there is  Norway and Slovakia in country/region variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6d12cd-5b37-4df7-acab-7337390a21de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking country column\n",
    "print(df_recovered['Country/Region'].isin(['Norway']).any())\n",
    "print(df_recovered['Country/Region'].isin(['Slovakia']).any())\n",
    "\n",
    "\n",
    "#Checking state column\n",
    "\n",
    "print(df_recovered[\"Province/State\"].isin([\"Slovakia\"]).any())\n",
    "print(df_recovered['Province/State'].isin(['Norway']).any())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc09936e-12d0-447e-b3b1-4c5939aa9b66",
   "metadata": {},
   "source": [
    "We did find both countries in Country/Region, Whereas we didnt find any of the countries of interest in Province/State. This means again that identifier will be Country/Region. I will confirm that dates are integers in subsequent code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27bd60c-efda-4227-bc73-9c09cffa9097",
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered_dates = df_recovered.iloc[:,4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7acee0-b8ab-4108-a017-56c1039b25be",
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered_dates.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f56a582-aa37-4a70-8ad6-c795cfe895be",
   "metadata": {},
   "source": [
    "All dates are integers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd057326-d23c-4dff-8647-10dfc6acb85c",
   "metadata": {},
   "source": [
    "### 4.3.2 Cleaning the dataset with recovered cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaed53d4-5f6c-46b8-874f-aa505fcd73dd",
   "metadata": {},
   "source": [
    "I will filter the rows dataset applying boolean mask to relevant rows and columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a054de-1270-454a-9f46-dce47d3c0dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recoveredd =df_recovered[(df_recovered['Country/Region'] == 'Slovakia') | (df_recovered['Country/Region'] == 'Norway')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b8218e-61d6-418f-b04f-34d15eea989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reassigning the new table to the old one\n",
    "df_recovered = df_recoveredd\n",
    "df_recovered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d9dfc6-43d0-47f6-8fd9-5361196673f5",
   "metadata": {},
   "source": [
    "I need to change row indexes. I reset them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acd6bf6-363f-4441-892a-2b744f6d4f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recovered.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ab0ab6-28d1-46bd-b7f0-12ac855bdfc6",
   "metadata": {},
   "source": [
    "Now I  drop all the irelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aad038e-6743-4038-8996-bc82e79a04bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Droping unneccessarry columns\n",
    "df_recovered.drop(['Long', 'Lat', 'Province/State'], axis = 1, inplace = True)\n",
    "df_recovered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7bb0dd-bc4a-4296-8166-7b69aa87719d",
   "metadata": {},
   "source": [
    "##### -----Cleaning successfull! -----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1e2a62-d4ff-4dcd-9f6a-9a6b1e0eefc6",
   "metadata": {},
   "source": [
    "## 4.4 Checking the tables all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265700e8-77b3-41ee-b527-8198e4f2a4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f28e2cf-3f15-4e6e-bc04-cf40219c037b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cdcc46-1869-4db5-bff4-9720a6f6ef56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recovered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf635b8-3a63-4f97-ad98-a1e99afa8835",
   "metadata": {},
   "source": [
    "All the tables seem to be correctly filtered, I confirm that by printing out datatypes in columns of respective dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9953b9bb-48ff-40ae-937e-c60f461b6f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_recovered.info())\n",
    "print(df_deaths.info())\n",
    "print(df_cases.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af13d529-47bc-462c-9b64-b097b6611fee",
   "metadata": {},
   "source": [
    "All the dataframes have 445 integers(dates) and 1 object(identifier). I will finally confirm a cleanliness of dataset viewing their shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a35ebff-489f-4f2e-95cc-27da6a9dcc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_recovered.shape)\n",
    "print(df_deaths.shape)\n",
    "print(df_cases.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58620eba-2869-4c83-b3b2-98428b1228b4",
   "metadata": {},
   "source": [
    "All of them have two rows(Slovakia and Norway) and 446 columns out of which 445 are date columns and 1 is identifier. I confirm dataset is ready for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f61b64-173d-4789-8b69-bd8aea1a3c6a",
   "metadata": {},
   "source": [
    "# 5 Melting the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c005368-6be3-4e6e-ac92-a02962c04f93",
   "metadata": {},
   "source": [
    "## 5.1 Melting the cases dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d720e1-2839-489c-9c5e-c7d6b9c3fb03",
   "metadata": {},
   "source": [
    "Melting means that I change format of the table. Instead of having 446 columns, I want to have just three. these are:\n",
    "country/region, date and number of cases.The table will be long because each date column will be listed as row twice- one for slovakia and same date for norway. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd096ef0-b837-4b84-a823-8aa4e524ee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases = df_cases.melt(id_vars = 'Country/Region', var_name = 'date', value_name = \"cases_number\").copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659b9cda-e599-4295-80f8-79c14b6d0c2d",
   "metadata": {},
   "source": [
    "Checking the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5c566b-82c7-41dc-b77a-30d707058d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc78bd5-0a52-4d62-8084-3e2c6ad5a0c5",
   "metadata": {},
   "source": [
    " ##### The table is melted correctly, there are 890 rows which is 455 per country this number of rows corresponds to number of date columns in the original dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fc10f6-f70e-429c-99fd-5ff8f6b881ca",
   "metadata": {},
   "source": [
    "## 5.2 Melting the deaths dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354a361b-c764-4e6f-9ec6-2ec8446f8b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deaths = df_deaths.melt(id_vars = \"Country/Region\", var_name = 'date', value_name = 'deaths_number').copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72abc0e-3ca0-4d4d-a1e8-d97840e0fa13",
   "metadata": {},
   "source": [
    "Checking the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce99187-b8cf-4725-a2b8-2eeda5dc952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deaths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1319b11-7e3f-4e27-84b6-5d1cd4c1e47e",
   "metadata": {},
   "source": [
    " ##### I confirm that columns are named correctly and shape of the table 890 x 3 is desirable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a72ef9-164f-45e6-822f-7c003e5b551b",
   "metadata": {},
   "source": [
    "## 5.3 Melting the dataset with recovered cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9b4c44-aa91-43bc-8a36-24f86fae8655",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recovered = df_recovered.melt(id_vars = \"Country/Region\", var_name = \"date\", value_name = 'recovered_number').copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1995bcd1-2e5a-457d-b559-d33af2cde099",
   "metadata": {},
   "source": [
    "Checking the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad86ab78-ed07-4a3a-a8a4-79533ce19834",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recovered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f431281c-bf87-442b-94f9-90b19186c0c8",
   "metadata": {},
   "source": [
    "I confirm that table shape matches perfectly the table shapes of recovered and cases tables. All three tables begin and finish with the same date. Lastly i check if there are any duplicates in the date columns. Each dataset should not have more than 2 duplicates of date(corresponding to 2 respective countries) in itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fa97f5-ee53-412a-a0fe-c2162c554df6",
   "metadata": {},
   "source": [
    "Accessing the number of duplicates for each date in each dataset(correct should be 2 for each date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98414b8c-1cec-45f8-a77e-5b148eb13039",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_duplicates_size = df_cases.groupby(\"date\").size()\n",
    "deaths_duplicates_size = df_deaths.groupby(\"date\").size()\n",
    "recovered_duplicates_size = df_recovered.groupby(\"date\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8db6414-8be3-4315-b73f-2aa3873ed3e8",
   "metadata": {},
   "source": [
    "Checking if each date contains no more or no less than two duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a842aee2-2bba-4ffa-bdeb-8a10e32d8e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print((cases_duplicates_size == 2).all())\n",
    "print((deaths_duplicates_size == 2).all())\n",
    "print((recovered_duplicates_size == 2).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3c6f32-3aee-4e8b-aff0-6813e49e478f",
   "metadata": {},
   "source": [
    " ##### Melting successfull!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1531c43b-1255-4819-930c-0f5962f2b532",
   "metadata": {},
   "source": [
    "# 6. Merging datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66525832-b53f-46da-92a4-9efc7e8e523c",
   "metadata": {},
   "source": [
    "I will merge the datasets using \"LEFT JOINs\". Since the individual datasets (\"df_cases\", \"df_deaths\", \"df_recovered\") are already clean and perfectly aligned with 890 rows each for Norway and Slovakia over time, the final merged dataset should also retain this length (890 rows) if the joins are performed correctly.\n",
    "\n",
    "While two simple \"LEFT JOIN\"s would suffice for this specific merging task, I want to demonstrate proficiency in more in-demand SQL skills for my portfolio. Therefore, I will structure these joins using a Common Table Expression (CTE) and a subquery. This approach showcases my ability to write modular, readable, and more complex SQL queries, which is a valuable skill for data analysis roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b35228-5620-4755-b5b0-ca4ba99c88a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merging_query = \"\"\" WITH recovered_join  AS (SELECT \n",
    "                            \"Country/Region\",\n",
    "                            \"date\", \n",
    "                            \"recovered_number\" \n",
    "FROM df_recovered)\n",
    "\n",
    "SELECT c.\"Country/Region\" AS country,\n",
    "       c.\"date\", \n",
    "       c.\"cases_number\", \n",
    "       d.\"deaths_number\", \n",
    "       r.\"recovered_number\"\n",
    "FROM df_cases AS c\n",
    "\n",
    "LEFT JOIN (SELECT \"deaths_number\", \n",
    "                  \"date\", \n",
    "                  \"Country/Region\"\n",
    "           FROM df_deaths) AS d\n",
    "on c.\"date\" = d.\"date\" AND c.\"Country/Region\" = d.\"Country/Region\"\n",
    "              \n",
    "LEFT JOIN recovered_join AS r \n",
    "on c.\"date\" = r.\"date\" AND r.\"Country/Region\" = c.\"Country/Region\";  \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7146070-c025-43d3-979f-186c2c5f4ef5",
   "metadata": {},
   "source": [
    "Turning query into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e879399-4837-49db-bebd-9ac41ce15de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timeseries = duckdb.query(merging_query).to_df()\n",
    "df_timeseries "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3f2de6-f3eb-4ee4-8e9d-e5d2831e6c2f",
   "metadata": {},
   "source": [
    "## 6.1 Verifying integrity of the time series dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbe861a-266b-42fb-b4c3-7ff22cc5b10c",
   "metadata": {},
   "source": [
    "Table df_timeseries  contains 890 rows and five columns which is expected outcome of merging.Finally I check missing values. if there are none it means merging was succesfull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3b3282-0edc-44be-8506-7b8c4dc681fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_timeseries .isnull().any())\n",
    "print(df_timeseries.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4248a0-8753-4be6-a3a6-1a1b40359c7d",
   "metadata": {},
   "source": [
    "##### ----No missing values found so merging went well----\n",
    "I see that date is coded as object and I need to convert it to date-time format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8cd5a5-595f-4d56-967a-340ccef5eb99",
   "metadata": {},
   "source": [
    "# 6. Date-time conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45151a6a-1100-4c6c-b85e-7a3056a51530",
   "metadata": {},
   "source": [
    "I pluck first row with date column first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a94a411-ae6b-41ed-ab97-098fb04e35ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf0b5af-0d32-4df8-a786-f0988706e3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type((df_timeseries.iloc[0, 1])))\n",
    "print(df_timeseries.loc[0, \"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc894c2-a329-479b-998c-522e8bd54f64",
   "metadata": {},
   "source": [
    "It is string format, but i need date - time format. I continue with  changing  cariable string format to date-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c586061e-b951-4068-80f5-1525aed4bb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timeseries[\"date\"] = pd.to_datetime(df_timeseries[\"date\"], format = \"%m/%d/%y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a06071-0c42-4f7f-96f8-dd0766c47a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91a025a-b249-4cd8-aeb0-f935ddc01be4",
   "metadata": {},
   "source": [
    "looks correct but I will sort it by date and check type of date column once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67ea3bf-62d7-43e7-9f48-018896b28879",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df_timeseries.loc[0, \"date\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a4a806-003a-490e-aab6-e347fda1a4d5",
   "metadata": {},
   "source": [
    "It is date-time format! Now i sort the  dataset based on date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a05731-2680-43fa-935c-22c93bc7dd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timeseries = df_timeseries.sort_values(\"date\").copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba628ad2-d8af-4991-a1e6-587497602597",
   "metadata": {},
   "source": [
    "Checking the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cdf2c5-4568-449a-ae40-d12dac5002a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07705a59-58ad-4505-922c-eb19e99b6af1",
   "metadata": {},
   "source": [
    "## 6.1 Reset index of the sorted dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e986eb76-7467-4e6d-83b8-c10683285d31",
   "metadata": {},
   "source": [
    "Dataset is now ordered by the date but I need to get the index right as well. I will therefore make date as an  index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dae493-cef7-4149-a232-d47c1813fa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timeseries.set_index(\"date\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b988974-e8cc-44c5-b1bf-7198a4af29d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f128172-ca66-4767-9abd-dc431c0dc2e9",
   "metadata": {},
   "source": [
    "Checking the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617cf682-6265-478d-bd30-2f277b15071a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timeseries.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161320dc-ec10-4a56-a6b5-268534d452ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timeseries.loc[\"2021-04-10\",:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8821df-5dba-4475-b4ea-e985334d867e",
   "metadata": {},
   "source": [
    "I can use date to access rows and dates are listed ar indexes which is desirable.\n",
    "##### ---Dataset is sorted, correctly indexed and ready for explanatory data analysis(EDA)---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f33337-22cd-4c61-8d63-a9b40da4bd0d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8515f44-bd1e-400d-a1a5-52fe91fcda92",
   "metadata": {},
   "source": [
    "# 7 Explanatory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a42e2d-7ff3-4f2c-86d8-a499f12c6604",
   "metadata": {},
   "source": [
    "## 7.1 Exploring the  timeframe of the dataset as a whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e75677-d96f-448b-a770-17aca05a7a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing starting date\n",
    "print(df_timeseries.iloc[1, :])\n",
    "#Printing end date\n",
    "print(df_timeseries.iloc[-1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc0bf37-c3e3-4891-a830-3a2b7ca16929",
   "metadata": {},
   "source": [
    "We will conduct the time series analysis within the period between the dates 2020-01-22 and 2021-04-10 which will give us almost one year and three months window to look at. This is enough time to access aspects such as Trend, Seasonality, Stationarity, Autocorrelation, Noise, Outliers, Frequency, Structural Breaks, Cyclic Patterns, Volatility. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6950860f-3428-47c6-96b6-4f2de9a6a65a",
   "metadata": {},
   "source": [
    "## 7.2 Explanatory data analysis of cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49693ae-6a5a-47f7-bfbe-3f83941c9ba9",
   "metadata": {},
   "source": [
    "First I check average number of cases per week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed27bd2-a9cc-459b-86ba-7842bc698c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekly_avg = (\n",
    "    df_timeseries\n",
    "    .groupby(\"country\")\n",
    "    .resample(\"W\")\n",
    "    .mean(numeric_only = True)\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e977dc6-d126-4fb5-885d-4e1029cff539",
   "metadata": {},
   "source": [
    "Index of weekly_avg table shows weeks in chronological order and under cases_number,\tdeaths_number, and recovered_number, we have weekly averages of those metrics per week. Now I am going to find week with the most average cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f432e6-185c-48f5-a748-8c7af3a7978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering maximum average per week for Slovakia\n",
    "print(df_weekly_avg[(df_weekly_avg[\"country\"] == \"Slovakia\")][\"cases_number\"].max())\n",
    "#Filtering maximum average per week for Norway\n",
    "print(df_weekly_avg[(df_weekly_avg[\"country\"] == \"Norway\")][\"cases_number\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48ef143-a0f5-466c-9b9f-228b18f9e84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the week with the most average cases in Slovakia\n",
    "df_weekly_avg[(df_weekly_avg[\"country\"] == \"Slovakia\") & (df_weekly_avg[\"cases_number\"] >= 367726)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97685e4-fdbd-46a8-9435-4e450cf69e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the week with the most average cases in Norway\n",
    "df_weekly_avg[(df_weekly_avg[\"country\"] == \"Norway\") & (df_weekly_avg[\"cases_number\"] >= 101355)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef90426-6410-4010-a3a3-d81920306a87",
   "metadata": {},
   "source": [
    "Both Norway and Slovakia had the most average cases number in the  end of the investigated period. Specifically the week 2021-04-11. Slovakia had more than three-fold more cases on average in this week(367727) compared to Norway (101355) at this point of time. While I am not calculating incident or death rates here, it's worth noting that both countries have very similar population sizes (around 5.4 million each), making this difference in raw average cases particularly striking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be013df-fb70-41d7-9dee-1c7476d7e15b",
   "metadata": {},
   "source": [
    "Now I visualize the  COVID-19 cases in both countries  with lineplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27459a9d-392e-40c1-9631-674800bd959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_plot = sns.lineplot(x = \"date\", y = \"cases_number\", data = df_timeseries, hue = \"country\")\n",
    "cases_plot.set_title(\"Covid-19 cases between country comparison\", y = 1.03)\n",
    "cases_plot.set(xlabel =\"Moment in time\", ylabel = \" Number of cases\")\n",
    "plt.xticks(rotation = 45)\n",
    "sns.figsize = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7badafe-4704-49c8-a0e7-7dbf6950ed1e",
   "metadata": {},
   "source": [
    "The figure gives us a quite nice overview, but before I comment on it I will try to smoothen it out by rolling averages per week. I will use a window function in SQL to do it and i need to move the date to a column from index. In this step I will do the same thing for deaths_number and recovered_number so I will not have to do it over and over again three times. First I will reset index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e143e0ea-340d-4184-895f-0fb8d8795159",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timeseries.reset_index(inplace = True)\n",
    "df_timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac6243d-9dc7-4c0a-8b2e-dfab2fb0ca93",
   "metadata": {},
   "source": [
    "Done!\n",
    "Now I can proceed with querying a window function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04d5b76-26e0-4af9-a75f-0dbd7c9a73ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rollingweek_cases = \"\"\" SELECT *, \n",
    "                               AVG(cases_number) \n",
    "                               OVER (PARTITION BY country \n",
    "                               ORDER BY date \n",
    "                               ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) \n",
    "                               AS rolling_weekly_cases, \n",
    "                           \n",
    "                              AVG(deaths_number) \n",
    "                              OVER(PARTITION BY country \n",
    "                              ORDER BY date \n",
    "                              ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) \n",
    "                              AS rolling_weekly_deaths, \n",
    "                          \n",
    "                              AVG(recovered_number) \n",
    "                              OVER(PARTITION BY country \n",
    "                              ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW)\n",
    "                              AS rolling_weekly_recovered \n",
    "                        FROM df_timeseries\"\"\"\n",
    "df_timeseries = duckdb.query(df_rollingweek_cases).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd364b8-307a-404f-b3e0-ab61271c32ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4346c266-5be5-48ec-9e9d-eaf5e9dfcfc9",
   "metadata": {},
   "source": [
    "All looks fine. Now I will create smoother plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfd6fd2-5aa1-4cf1-9ff1-44acd15a8ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating line-plot\n",
    "weekly_cases_avg_plot = sns.lineplot(x = \"date\", y = \"rolling_weekly_cases\", data = df_timeseries, hue = \"country\")\n",
    "#Customizing the line-plot with average cases\n",
    "weekly_cases_avg_plot.set_title(\"Covid-19 cases weekly average\", y = 1.03)\n",
    "weekly_cases_avg_plot.set(xlabel =\"Moment in time\", ylabel = \"Average number of cases per 7 days\")\n",
    "plt.xticks(rotation = 45)\n",
    "sns.figsize = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3789deb2-52d8-45b8-b9e3-6ed684402276",
   "metadata": {},
   "source": [
    "The line plot shows that Norway had a higher number of average cases until approximately the first half of the investigated period. Around October 2021, Slovakia exceeded Norway in the number of average cases per week, and for the remainder of the period shown, Slovakia's average number of cases per week grew almost exponentially, whereas in Norway, the curve is more flattened, indicating gentler growth compared to Slovakia. I will check weekly in percentage points for both countries and volatility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43345ac6-e77b-4d3b-89a4-d6631f497ad1",
   "metadata": {},
   "source": [
    "Reseting index back to date-time after querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a476c8-9bc3-4a0e-8e0d-9bee06641578",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timeseries.set_index(\"date\", inplace = True)\n",
    "df_timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1185d50-d0f4-4573-8359-359824db9c27",
   "metadata": {},
   "source": [
    "Now I check differences in average number of cases between the weeks in Norway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782e1818-7b26-4576-a515-90b72a5a261c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 900)\n",
    "df_weekly_avg[df_weekly_avg[\"country\"] == \"Norway\"][\"cases_number\"].pct_change() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae84755-b001-4fad-b5c6-120c71de1d56",
   "metadata": {},
   "source": [
    "The first entry (2020-01-26) for the percentage change is NaN because there is no preceding value to compare it to. The subsequent four entries are also NaN, indicating zero cases or no change from zero in those initial weeks. The inf (infinity) value on 2020-03-01 precisely indicates the first week where Norway recorded a non-zero number of average COVID-19 cases, leading to a division by zero in the percentage change calculation.\n",
    "\n",
    "After these initial weeks, the weekly increase in cases stabilized, fluctuating between approximately 0 and 8 percentage points until the end of October 2020. This was followed by a period of higher fluctuation for about five subsequent weeks, with increases ranging between 11 and 19 percentage points. After the end of November 2020, the percentage point increase in cases stabilized again, generally ranging from 2% to 9% until April 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098464de-fbac-46b8-a598-7bf7ca794385",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekly_avg[df_weekly_avg[\"country\"] == \"Slovakia\"][\"cases_number\"].pct_change()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a362e3-1106-47b3-bd89-4504cad3497d",
   "metadata": {},
   "source": [
    "Similarly to Norway, Slovakia had no COVID-19 cases present until March 2020. In April 2020, the first outbreak resulted in a significant percentage point increase over the subsequent seven weeks. From mid-May to mid-July, the percentage point increase in cases did not exceed eight percentage points. However, at the beginning of August, the percentage point increase in COVID-19 cases gradually rose and peaked in mid-October, with the annual weekly percentage point increase reaching almost 52%. By mid-January 2021, the weekly increase in COVID-19 cases had dropped below a 10% threshold and gradually decreased to 1.5% in the final week of our period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d909a107-7ddc-4bd2-b077-7d1a2257309a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

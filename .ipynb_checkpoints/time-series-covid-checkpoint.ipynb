{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1380ec3-68d6-4251-b33f-cfafe5e201f2",
   "metadata": {},
   "source": [
    "# 1 Introduction and clinical context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4860c4-68e9-4e89-bb6c-9cc495cf22af",
   "metadata": {},
   "source": [
    "This project presents a time series prediction of COVID-19 deaths using the ARIMA (Autoregressive Integrated Moving Average) model, a robust statistical approach widely utilized in epidemiological forecasting. Accurate prediction of COVID-19 mortality trends is critical for guiding public health responses and resource allocation, particularly amidst unpredictable pandemic waves [1–4].\n",
    "\n",
    "ARIMA models have been successfully applied in recent studies to estimate death rates across diverse regions, including major outbreaks in countries such as India, Israel, Japan, and the United States. Scientific analyses demonstrate that ARIMA performs well when capturing short-term patterns in non-seasonal time series data, using past values and error components for reliable forecasting. The model's prediction accuracy is frequently validated with metrics such as RMSE (Root Mean Square Error), AIC (Akaike Information Criterion), and BIC (Bayesian Information Criterion) [2,5–6,4].\n",
    "\n",
    "In comparison to other machine learning and statistical methods—such as LSTM and Random Forest—ARIMA models provide a transparent and interpretable framework especially suited for data-driven public health decisions. For pandemic forecasting, past studies report lower RMSE scores for ARIMA versus SARIMA in predicting COVID-19 deaths, underscoring its effectiveness for mortality rate projections [7,6,1,5].\n",
    "\n",
    "From 1 January 2020 to 31 December 2024, mortality in Norway was 46,054, corresponding to a total mortality rate of 4.7% during this period [8]. In this project, I compared mortality patterns in Slovakia — my country of origin — and Norway, where I currently live.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837603f9-98c3-43cc-a00a-10768720e757",
   "metadata": {},
   "source": [
    "# 2  Methods and hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79795acc-fa91-4ef7-b411-74a0676f675d",
   "metadata": {},
   "source": [
    "I hypothesized that the number of COVID-19 cases would be a significant predictor of the number of deaths in the subsequent one, two, and three weeks.\n",
    "\n",
    "I compared mortality and incidence between the two countries for the period 1 January 2020 to 10 April 2021. First, I applied rolling weekly averages to smooth the mortality and incidence curves. I then visualized daily differences in mortality and incidence between the countries, which I subsequently aggregated into weekly differences.\n",
    "\n",
    "Next, I focused on forecasting weekly deaths. I performed seasonal decomposition and implemented exponential smoothing. After assessing stationarity, I applied ARIMA, SARIMA, and ARIMAX models (with COVID-19 cases as an exogenous variable).\n",
    "\n",
    "Additionally, I tested models using deaths lagged by one, two, and three weeks as predictor variables, and finally, I included all lagged variables in a combined model. Model performance was evaluated using mean absolute error (MAE), root mean squared error (RMSE), and mean absolute percentage error (MAPE).\n",
    "\n",
    "For all ARIMA models, I assessed residual diagnostics using the Ljung–Box (L1) and Jarque–Bera (JB) tests. I also visualized residual distribution heterogeneity and autocorrelation using Q–Q plots, ACF and PACF plots, and histograms in models which were.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2813cd73-9773-48ee-bf5b-c6f432724636",
   "metadata": {},
   "source": [
    "# 3 Import of data and libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a6f835-69c6-4839-888a-cac11becc2ee",
   "metadata": {},
   "source": [
    "## 3.1 Import of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc793ca8-df34-4fa3-9f93-eb5e84594b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing basic data analyst libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import kurtosis, norm\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#Importing libraries for SQL querrying\n",
    "import duckdb \n",
    "\n",
    "#time-series specific libraries\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.holtwinters import  SimpleExpSmoothing, ExponentialSmoothing\n",
    "!pip install pmdarima\n",
    "from pmdarima import auto_arima, ARIMA, model_selection\n",
    "\n",
    "#Other libraries\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error, mean_absolute_percentage_error\n",
    "import warnings\n",
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d3603c-6982-4308-82b7-f144456b5a09",
   "metadata": {},
   "source": [
    "## 3.2 Import of tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d98b1a6-2877-4596-8313-9e1867f18c41",
   "metadata": {},
   "source": [
    "I imported all relevant tables for analysis at this stage, treating them as a database. I will examine them in detail later, after cleaning and querying in separate stages. I chose this approach for logical organization and reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd34dc1-8988-482c-80ff-78f6dc9c6914",
   "metadata": {},
   "source": [
    "### 3.2.1 Importing confirmed cases table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661943cf-0e35-4eed-a45c-1802dd7ad2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases = pd.read_csv('data/raw/confirmed_global.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389d255f-b2b1-47f5-a01d-dc9f4d52a686",
   "metadata": {},
   "source": [
    "Verifying import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7979b4f-3849-4ae5-8b43-54d506e81b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cases.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba95d563-b0fe-4301-9f97-e7ef7ce91214",
   "metadata": {},
   "source": [
    "##### ---Cases dataset imported successfully---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7406c85-43d8-474d-b076-1f6a0d66bf54",
   "metadata": {},
   "source": [
    "### 3.2.2 Importing death cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e9a1e5-fbde-4846-86c4-74ad856c91f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deaths = pd.read_csv('data/raw/deaths_global.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee17518-fdf9-415f-a225-30aa54526cdc",
   "metadata": {},
   "source": [
    "Verifying import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9ebbd4-552f-4eae-bf0a-3723c94e98c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_deaths.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cff8e2-9fd7-4a8b-8031-061c217c12cf",
   "metadata": {},
   "source": [
    "##### ----Deaths dataset importet successfully----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e44fc3-30f3-49f7-ba8d-8db9fcb41d6a",
   "metadata": {},
   "source": [
    "##### The datasets have dates as rows and for the time series analysis, they need to be be transformed into rows. This process is called melting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e578c79-44d4-4e1d-a70d-b81e4e0f90c7",
   "metadata": {},
   "source": [
    "# 4 Inspecting cleaning and preparing datasets for melting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709aab47-3590-4e05-98ec-f4ed0969c869",
   "metadata": {},
   "source": [
    "In this sections I will get acquianted with dataset, melt it, deal with missings and prepare it for melting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9734eb-32f6-4d84-a8ca-b753ebb5e81b",
   "metadata": {},
   "source": [
    "## 4.1 Inspecting cleaning and preparing the dataset with cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7182554c-3b5d-48f9-827a-0bd3264a4e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cases.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b2d98c-5c61-462b-a0c4-2f212d9ae31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cases.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275cbe7b-52aa-44ce-9bb8-1f1e1be53508",
   "metadata": {},
   "source": [
    "Shape of the dataset is 274 rows and 449 columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646d4aa3-da1f-42d7-8be5-dc54897184d0",
   "metadata": {},
   "source": [
    "From the first glance I see identifier columns such as province/state, Country/Relogion, Latitude and longitude. There are 449 columns where majority corresponds to individual dates within given period of time. To confirm that only date columns follow 'Long', and that there are no unexpected columns in between, I will iterate through the column names and count the columns to crosscheck whether i itterate through columns correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81382358-5877-4456-aef9-181a342ba34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for col in df_cases:\n",
    "    print(col)\n",
    "    count = count + 1\n",
    "    \n",
    "print(\"the column count is: \", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3fc440-9597-420c-b431-05b0bd542932",
   "metadata": {},
   "source": [
    "All the respective columns  after 'Long' seem to represent individual dates and column count from the loop is in agreement with the column count from df_cases.head(). Next I need to check data types of individual columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab53c401-eb67-4b49-b574-605532f13000",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_integer = 0\n",
    "count_other_datatypes = 0\n",
    "for col in df_cases:\n",
    "    print(df_cases[col].dtype)\n",
    "    if df_cases[col].dtype == int:\n",
    "        count_integer = count_integer +1\n",
    "    else:\n",
    "        count_other_datatypes = count_other_datatypes + 1\n",
    "\n",
    "print('Integer count is:', count_integer, '\\n Other data types count is:', count_other_datatypes)\n",
    "if count_integer == 445:\n",
    "    print('All date-columns are integers!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a4c21b-f856-4945-aacb-2991efef5148",
   "metadata": {},
   "source": [
    "Loop confirmed that all date columns are formated as an integer datatype. I see that other datatypes in the dataset are objects and float. I will confirm it with command .info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac9d221-0a57-4ca4-ab30-a45762262f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16472b40-830f-46e9-8947-9750ff32ca4c",
   "metadata": {},
   "source": [
    "Verification showed exactly those datatypes i noticed  above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcdc72e-35bb-4f54-8419-97ce6fe2b866",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cases.describe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e5b01c-f0a5-4cd6-a6ed-87f047fe428d",
   "metadata": {},
   "source": [
    "Describe command gives me an overview over the dataset. There are a lot of missing values in province/state. I have to investigate this variable closer. Country/region column will likely be the variable of interest and will serve us as identifier, whereas latitute and longitude are not interesting for our analysis. Even though some viruses are known to spread better in colder or warmer climates, Cross-country comparisons in cases between the countries are not feasible because we don´t have data on amount people tested and countries were very different in test measures and  test equipment. In this dataset we lack the data for it. Quick overview shows that there were 0 cases in a begining whereas number of cases began to increase and fluctuate over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b5aba2-4264-4aaa-a36c-31ace31f99e3",
   "metadata": {},
   "source": [
    "### 4.1.2 Cleaning the dataset with cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8056fb-3ede-41c7-b50c-3244089b1b18",
   "metadata": {},
   "source": [
    "I am checking if there are any missing values in columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83fdd91-f8ca-40e7-91c3-f99ef7eb70d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases.isna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6721e66d-92a9-41e2-860d-5bc2d7065653",
   "metadata": {},
   "source": [
    "I cannot see the whole dataset, but the output indicates that Province/state has missing values. I am going to investigate it further along with other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5686179-6111-47bb-b765-4f29b6944d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcf170c-f367-41c4-9762-3d2cb4c5c6aa",
   "metadata": {},
   "source": [
    "It seems that longitude latitude and province contain misssing values whereas dates seem to contain no missing value. I need to check my assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b9bf89-fed4-431e-b389-73af1e266a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases.isna().any().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb40f18a-4920-4760-a138-3b65e5fbeafb",
   "metadata": {},
   "source": [
    "There are three columns in the df_cases with missing values. Those are Province/State, Lat, Long.\n",
    "Before I go further with cleaning and droping, want to check which column is the most pertinent one to become identifier column during melting. The two most valiable options are Province/state and Country/Region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa707e22-97f6-4821-8cd9-4d82ae058e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases[['Province/State', 'Country/Region']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9297f0f-3c30-496c-a8ba-731855f6745c",
   "metadata": {},
   "source": [
    "I confirm that column 'Province/State' is of no interest as it shows either overseas territories and provinces/states within a few countries.Contrary, Norway and Slovakia are both within Country/Region column which will become our identifier. Now I will strip the dataset from all the unneccesarry columns and rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a71ed0-e4ea-469e-b3f0-17a3f478896e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases_filtered = \"\"\"\n",
    "SELECT *\n",
    "FROM df_cases\n",
    "WHERE \"Country/Region\" = 'Norway' OR \"Country/Region\" = 'Slovakia';\n",
    "\"\"\"\n",
    "df_cases = duckdb.query(df_cases_filtered).to_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b91bbed-0f7f-48b2-8f48-ed09b649830f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Checking whether i filtered rows correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3c87a4-0424-476d-863f-36769166ffc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cases.iloc[:, 1:])\n",
    "type(df_cases), df_cases.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c70464-1fb2-4beb-88e1-1fedb8007324",
   "metadata": {},
   "source": [
    "I correctly filtered just relevant rows and transformed queried data into dataframe. There are two rows- Slovakia and Norway with number of columns(449) remained unchanged. Now I will proceed with dropping all the unneccessarry columns. namely: \n",
    "'Province/State', \n",
    "'Long', \n",
    "'Lat'\n",
    "\n",
    "As mentioned earlier, variables lattitude and longitude are of  no value in our analysis, whereas province/state is variable indicating a region within larger countries. Countries of interest are Slovakia and Norway and we have no addotional data on specific locations within those countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40558ed-5126-42fd-8d1c-31c9013f7cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases.drop(['Province/State', 'Long','Lat',], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0270844-3fbd-459b-892c-3ef612393007",
   "metadata": {},
   "source": [
    "Checking if there are some missing values in the dataset:\n",
    "df_cases_filtered.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a50a7e-56fe-4199-9e1f-31e28f0095e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases.isna().any().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834ded40-89e3-4186-8f2b-41302710ac11",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "I correctly dropped all the unneccessarry rows and dataset is ready to melt. But before I will do it I will prepare datasets with deaths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108ab597-2659-4fbb-ae4d-e7815a21a4d5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## 4.2 Inspecting cleaning and preparing the dataset with deaths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f65c8d-5045-4faa-b1c9-d1d6c28c0d6e",
   "metadata": {},
   "source": [
    "### 4.2.1 Inspecting the dataset with deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cc65d2-f510-49f0-ad2f-fdb5b56538cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_deaths.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc26e09-f9bd-4ac9-95d0-6a92340aa920",
   "metadata": {},
   "source": [
    "The deaths dataset has the same number of rows(274) and columns (449) as the cases dataset. its a good indication because the process of cleaning checking and extracting of data will be very simmilar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f619f3-a67e-4ed7-be33-9563ec7e2c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_deaths.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b80890-6985-4736-bf4a-5104ff9e787a",
   "metadata": {},
   "source": [
    "Again, earlier dates indicate zero deaths whereas number of deaths towards the later period is higher and steadily increasing whereas first five variables: 'Province/State' 'Country/Region 'Lat''Long' seem to be identical through all the datasets. I have to confirm this assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a950dd1-0450-4017-8aa2-dfa6ac45e0ec",
   "metadata": {},
   "source": [
    "I am checking what datatypes exist in the dataset deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bff74f1-632d-4944-a839-639948be60f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_deaths.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c47b04b-836b-4259-ae98-3062be5e81f8",
   "metadata": {},
   "source": [
    "df_deaths dataset seem to have identical types  two float columns (lat, long), two object columns (Province/State Country/Region) and 445 integer(date columns). I will verify it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fa9e4b-3cd7-4798-983c-a57b1ddb0a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_deaths['Country/Region'].dtype)\n",
    "print(df_deaths['Country/Region'].dtype)\n",
    "print(df_deaths['Long'].dtype)\n",
    "print(df_deaths['Lat'].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d26efbc-8ab9-4cd3-bfb8-a37072901422",
   "metadata": {},
   "source": [
    "I confirm my assumption about float and object columns, I will check the date(integer columns as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf00249-3ea6-43d2-bd3b-9848d3d11140",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering dataset so there are only dates\n",
    "df_deaths_only_date = df_deaths.iloc[:, 4:]\n",
    "#Creating loop to check which type the column is\n",
    "date_counter = 0\n",
    "for col in df_deaths_only_date:\n",
    "    print(df_deaths_only_date[col].dtype)\n",
    "    date_counter +=1 \n",
    "\n",
    "print(date_counter)\n",
    "print(df_deaths_only_date.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19438b9-05fb-4102-afad-646bfffdb84f",
   "metadata": {},
   "source": [
    "I confirm that the dataset contains 445 integers! I am finished with scrutinizing the dataset with deaths and I move further to clean it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2521ef72-685f-4390-af56-08e5e2407305",
   "metadata": {},
   "source": [
    "### 4.2.2 Cleaning the dataset with deaths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d514350-45d0-43a6-903a-b43ffa58c2c5",
   "metadata": {},
   "source": [
    "I first check whether columns contain missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab6d369-fa61-4afa-8557-38162be741d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_deaths.isna().any())\n",
    "df_deaths.isna().any().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4171c175-d511-4661-94eb-1a382c37ff82",
   "metadata": {},
   "source": [
    "#I will check how many columns with missing values are present. If there are three we know they are those:\n",
    " Province/State      \n",
    " Lat       \n",
    " Long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bef2716-f5c8-43ee-974c-738b82d93ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deaths.isna().any().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f261c641-4976-4326-abda-92e1a7161f15",
   "metadata": {},
   "source": [
    "I confirm the assumption about the missing values. Now I check identifier columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9116861f-4988-449b-9edc-04b9acbc2910",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_deaths['Province/State'],df_deaths['Country/Region'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4956901-9207-47d8-8517-385d7c261344",
   "metadata": {},
   "source": [
    "Based on the output, I will drop the Long, Lat and Province/State columns as they are of no interest for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc533768-597b-458a-886d-4619a3f2982c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deaths.drop(['Province/State', 'Long', 'Lat'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d2d708-097e-4ed5-9ca9-08a42f8a0a6e",
   "metadata": {},
   "source": [
    "Filtering rows with Slovakia and Norway and re-writing original dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d276e53-c554-44ba-8eb3-47491acdd964",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deaths.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deb9e7e-54e1-4332-a449-2aa572730c5b",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d4cb18-e926-4a74-b03b-329f48a39b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#row-filtering\n",
    "df_deaths_filtered = \"\"\"\n",
    "SELECT * FROM df_deaths\n",
    "WHERE \"Country/Region\" = 'Slovakia' OR \"Country/Region\" = 'Norway' \"\"\"\n",
    "#re-writing df_deaths\n",
    "df_deaths = duckdb.query(df_deaths_filtered).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adc6966-a51e-4775-9e03-13049508a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' The dataset has', df_deaths.isna().any().sum(), 'missings', df_deaths.shape, df_deaths.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896cc2f5-188d-4b8a-a9ef-923b6c5627ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_deaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f56a582-aa37-4a70-8ad6-c795cfe895be",
   "metadata": {},
   "source": [
    "All dates are integers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00a9f19-9ee8-4b24-b35b-7f5e049277a9",
   "metadata": {},
   "source": [
    " ##### Dataset is correctly cleaned and filtered with dates as integers, identifier column som object and zero missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1e2a62-d4ff-4dcd-9f6a-9a6b1e0eefc6",
   "metadata": {},
   "source": [
    "## 4.3 Checking the tables all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265700e8-77b3-41ee-b527-8198e4f2a4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f28e2cf-3f15-4e6e-bc04-cf40219c037b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deaths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf635b8-3a63-4f97-ad98-a1e99afa8835",
   "metadata": {},
   "source": [
    "The tables seem to be correctly filtered, I confirm that by printing out datatypes in columns of respective dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9953b9bb-48ff-40ae-937e-c60f461b6f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_deaths.info())\n",
    "print(df_cases.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af13d529-47bc-462c-9b64-b097b6611fee",
   "metadata": {},
   "source": [
    "All the dataframes have 445 integers(dates) and 1 object(identifier). I will finally confirm a cleanliness of dataset viewing their shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a35ebff-489f-4f2e-95cc-27da6a9dcc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_deaths.shape)\n",
    "print(df_cases.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58620eba-2869-4c83-b3b2-98428b1228b4",
   "metadata": {},
   "source": [
    "All of them have two rows(Slovakia and Norway) and 446 columns out of which 445 are date columns and 1 is identifier. I confirm dataset is ready for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c005368-6be3-4e6e-ac92-a02962c04f93",
   "metadata": {},
   "source": [
    "## 5 Melting the cases dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d720e1-2839-489c-9c5e-c7d6b9c3fb03",
   "metadata": {},
   "source": [
    "Melting means that I change format of the table. Instead of having 446 columns, I want to have just three. these are:\n",
    "country/region, date and number of cases. The table will be long because each date column will be listed as row twice- one for slovakia and same date for norway. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd096ef0-b837-4b84-a823-8aa4e524ee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases = df_cases.melt(id_vars = 'Country/Region', var_name = 'date', value_name = \"cases_number\").copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659b9cda-e599-4295-80f8-79c14b6d0c2d",
   "metadata": {},
   "source": [
    "Checking the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5c566b-82c7-41dc-b77a-30d707058d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc78bd5-0a52-4d62-8084-3e2c6ad5a0c5",
   "metadata": {},
   "source": [
    " ##### The table is melted correctly, there are 890 rows which is 455 per country this number of rows corresponds to number of date columns in the original dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fc10f6-f70e-429c-99fd-5ff8f6b881ca",
   "metadata": {},
   "source": [
    "## 5.2 Melting the deaths dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354a361b-c764-4e6f-9ec6-2ec8446f8b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deaths = df_deaths.melt(id_vars = \"Country/Region\", var_name = 'date', value_name = 'deaths_number').copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72abc0e-3ca0-4d4d-a1e8-d97840e0fa13",
   "metadata": {},
   "source": [
    "Checking the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce99187-b8cf-4725-a2b8-2eeda5dc952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deaths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1319b11-7e3f-4e27-84b6-5d1cd4c1e47e",
   "metadata": {},
   "source": [
    " ##### I confirm that columns are named correctly and shape of the table 890 x 3 is desirable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1995bcd1-2e5a-457d-b559-d33af2cde099",
   "metadata": {},
   "source": [
    "Checking the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fa97f5-ee53-412a-a0fe-c2162c554df6",
   "metadata": {},
   "source": [
    "Accessing the number of duplicates for each date in each dataset(correct should be 2 for each date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98414b8c-1cec-45f8-a77e-5b148eb13039",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_duplicates_size = df_cases.groupby(\"date\").size()\n",
    "deaths_duplicates_size = df_deaths.groupby(\"date\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8db6414-8be3-4315-b73f-2aa3873ed3e8",
   "metadata": {},
   "source": [
    "Checking if each date contains no more or no less than two duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a842aee2-2bba-4ffa-bdeb-8a10e32d8e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((cases_duplicates_size == 2).all())\n",
    "print((deaths_duplicates_size == 2).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3c6f32-3aee-4e8b-aff0-6813e49e478f",
   "metadata": {},
   "source": [
    " ##### Melting successfull!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1531c43b-1255-4819-930c-0f5962f2b532",
   "metadata": {},
   "source": [
    "# 6. Merging datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66525832-b53f-46da-92a4-9efc7e8e523c",
   "metadata": {},
   "source": [
    "I will merge the datasets using \"LEFT JOINs\". Since the individual datasets (\"df_cases\", \"df_deaths\", ) are already clean and perfectly aligned with 890 rows each for Norway and Slovakia over time, the final merged dataset should also retain this length (890 rows) if the joins are performed correctly.\n",
    "\n",
    "While  a simple \"LEFT JOIN\"s would suffice for this specific merging task, I want to demonstrate proficiency in more in-demand SQL skills for my portfolio. Therefore, I will structure join with a subquery. This approach showcases my ability to write modular, readable, and more complex SQL queries, which is a valuable skill for data analysis roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b35228-5620-4755-b5b0-ca4ba99c88a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merging_query = \"\"\" \n",
    "SELECT c.\"Country/Region\" AS country,\n",
    "       c.\"date\", \n",
    "       c.\"cases_number\", \n",
    "       d.\"deaths_number\", \n",
    "FROM df_cases AS c\n",
    "\n",
    "LEFT JOIN (SELECT \"deaths_number\", \n",
    "                  \"date\", \n",
    "                  \"Country/Region\"\n",
    "           FROM df_deaths) AS d\n",
    "on c.\"date\" = d.\"date\" AND c.\"Country/Region\" = d.\"Country/Region\"\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7146070-c025-43d3-979f-186c2c5f4ef5",
   "metadata": {},
   "source": [
    "Turning query into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e879399-4837-49db-bebd-9ac41ce15de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timeseries = duckdb.query(merging_query).to_df()\n",
    "df_timeseries "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3f2de6-f3eb-4ee4-8e9d-e5d2831e6c2f",
   "metadata": {},
   "source": [
    "## 6.1 Verifying integrity of the time series dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbe861a-266b-42fb-b4c3-7ff22cc5b10c",
   "metadata": {},
   "source": [
    "Table df_timeseries  contains 890 rows and five columns which is expected outcome of merging.Finally I check missing values. if there are none it means merging was succesfull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3b3282-0edc-44be-8506-7b8c4dc681fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_timeseries .isnull().any())\n",
    "print(df_timeseries.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4248a0-8753-4be6-a3a6-1a1b40359c7d",
   "metadata": {},
   "source": [
    "##### ----No missing values found so merging went well----\n",
    "I see that date is coded as object and I need to convert it to date-time format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8cd5a5-595f-4d56-967a-340ccef5eb99",
   "metadata": {},
   "source": [
    "# 7. Date-time conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a94a411-ae6b-41ed-ab97-098fb04e35ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45151a6a-1100-4c6c-b85e-7a3056a51530",
   "metadata": {},
   "source": [
    "I check first row with date column first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf0b5af-0d32-4df8-a786-f0988706e3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type((df_timeseries.iloc[0, 1])))\n",
    "print(df_timeseries.loc[0, \"date\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc894c2-a329-479b-998c-522e8bd54f64",
   "metadata": {},
   "source": [
    "It is string format, but i need date - time format. I continue with  changing  cariable string format to date-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c586061e-b951-4068-80f5-1525aed4bb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timeseries[\"date\"] = pd.to_datetime(df_timeseries[\"date\"], format = \"%m/%d/%y\")\n",
    "df_timeseries.sort_values([\"date\", \"country\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a06071-0c42-4f7f-96f8-dd0766c47a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_timeseries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91a025a-b249-4cd8-aeb0-f935ddc01be4",
   "metadata": {},
   "source": [
    "looks correct but I will sort it by date and check type of date column once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67ea3bf-62d7-43e7-9f48-018896b28879",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df_timeseries.loc[0, \"date\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a4a806-003a-490e-aab6-e347fda1a4d5",
   "metadata": {},
   "source": [
    "It is date-time format! Now i sort the  dataset based on date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba628ad2-d8af-4991-a1e6-587497602597",
   "metadata": {},
   "source": [
    "Checking the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b789a0a-e64e-4a07-9232-985a4174a3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_timeseries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6ffbf5-77e9-40ac-9ae0-7cd8c0795356",
   "metadata": {},
   "source": [
    "##### ---Dataset is sorted, correctly indexed and ready for explanatory data analysis(EDA)---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8515f44-bd1e-400d-a1a5-52fe91fcda92",
   "metadata": {},
   "source": [
    "# 8 Explanatory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc0bf37-c3e3-4891-a830-3a2b7ca16929",
   "metadata": {},
   "source": [
    "We will conduct the time series analysis within the period between the dates 2020-01-22 and 2021-04-10 which will give us almost one year and three months window to look at. This is enough time to access aspects such as Trend, Seasonality, Stationarity, Autocorrelation, Noise, Outliers, Frequency, Structural Breaks, Cyclic Patterns, Volatility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e75677-d96f-448b-a770-17aca05a7a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing starting date\n",
    "print(df_timeseries.iloc[1, :])\n",
    "#Printing end date\n",
    "print(df_timeseries.iloc[-1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49693ae-6a5a-47f7-bfbe-3f83941c9ba9",
   "metadata": {},
   "source": [
    "First I check average number of cases per week. It will be interesting to see further in EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bb9c40-5b22-4e29-a9e3-fcbecb5b6a13",
   "metadata": {},
   "source": [
    "## 8.1 Between country comparisons incidence and mortality counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be013df-fb70-41d7-9dee-1c7476d7e15b",
   "metadata": {},
   "source": [
    "Now I visualize the  COVID-19 cases in both countries  with lineplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27459a9d-392e-40c1-9631-674800bd959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_plot = sns.lineplot(x = \"date\", y = \"cases_number\", data = df_timeseries, hue = \"country\")\n",
    "cases_plot.set_title(\"Covid-19 cases between country comparison\", y = 1.03)\n",
    "cases_plot.set(xlabel =\"Moment in time\", ylabel = \" Number of cases\")\n",
    "plt.xticks(rotation = 45)\n",
    "sns.figsize = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7badafe-4704-49c8-a0e7-7dbf6950ed1e",
   "metadata": {},
   "source": [
    "I will try to smoothen the figure out by rolling averages per week. I will use a window function in SQL to do it and i need to move the date to a column from index. In this step I will do the same thing for deaths_number and so I will not have to do it over and over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd364b8-307a-404f-b3e0-ab61271c32ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rollingweek_cases = \"\"\" SELECT *, \n",
    "                               AVG(cases_number) \n",
    "                               OVER (PARTITION BY country \n",
    "                               ORDER BY date \n",
    "                               ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) \n",
    "                               AS rolling_weekly_cases, \n",
    "                           \n",
    "                              AVG(deaths_number) \n",
    "                              OVER(PARTITION BY country \n",
    "                              ORDER BY date \n",
    "                              ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) \n",
    "                              AS rolling_weekly_deaths,\n",
    "                        FROM df_timeseries\"\"\"\n",
    "df_timeseries = duckdb.query(df_rollingweek_cases).to_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac6243d-9dc7-4c0a-8b2e-dfab2fb0ca93",
   "metadata": {},
   "source": [
    "Done!\n",
    "Now I can proceed with querying a window function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2d3b81-678e-42d0-89d9-7dd57523884b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4346c266-5be5-48ec-9e9d-eaf5e9dfcfc9",
   "metadata": {},
   "source": [
    "All looks fine. Now I will create smoother plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfd6fd2-5aa1-4cf1-9ff1-44acd15a8ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating line-plot\n",
    "weekly_cases_avg_plot = sns.lineplot(x = \"date\", y = \"rolling_weekly_cases\", data = df_timeseries, hue = \"country\")\n",
    "#Customizing the line-plot with average cases\n",
    "weekly_cases_avg_plot.set_title(\"Covid-19 cases weekly average\", y = 1.03)\n",
    "weekly_cases_avg_plot.set(xlabel =\"Moment in time\", ylabel = \"Average number of cases per 7 days\")\n",
    "plt.xticks(rotation = 45)\n",
    "sns.figsize = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3789deb2-52d8-45b8-b9e3-6ed684402276",
   "metadata": {},
   "source": [
    "The line plot indicates that Norway initially had a higher average number of weekly COVID-19 cases. However, around October 2020, Slovakia's case numbers rapidly increased and soon surpassed those of Norway. From that point onward, Slovakia experienced a sharp, near-exponential rise in cases, while Norway's case curve remained comparatively flatter. This suggests that Slovakia faced a more intense outbreak in the later stages of the period examined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bfb2e5-03f3-48d5-af10-072ab74df961",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating line-plot\n",
    "weekly_cases_avg_plot = sns.lineplot(x = \"date\", y = \"rolling_weekly_deaths\", data = df_timeseries, hue = \"country\")\n",
    "#Customizing the line-plot with average cases\n",
    "weekly_cases_avg_plot.set_title(\"Covid-19 deaths weekly average\", y = 1.03)\n",
    "weekly_cases_avg_plot.set(xlabel =\"Moment in time\", ylabel = \"Average number of deaths per 7 days\")\n",
    "plt.xticks(rotation = 45)\n",
    "sns.figsize = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29a000a-d942-4bb6-9e95-9f13686280b6",
   "metadata": {},
   "source": [
    "The rolling average visualization shows that Norway maintained relatively stable death rates, consistently below 1,000 per week. In contrast, Slovakia experienced a sharp increase in deaths starting in November 2020, closely mirroring the previous rise in cases. This one-month delay between rising cases and deaths highlights a typical lag observed in COVID-19 progression. The steep upward trend in Slovakia's death curve suggests the country may have faced an overwhelmed healthcare system, which likely contributed to higher fatality rates compared to Norway."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8c745f-a68a-4607-b592-8b6da435ead1",
   "metadata": {},
   "source": [
    "\n",
    "I will check weekly averages per week without rolling values. To do it I will do several steps. splitting dataset by country then truncate the daytime by week and calculate average per week. In addition, I will calculate daily, and weekly differences and their averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e42e7a3-b187-425f-b9c3-8074a66cd6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting values\n",
    "df_timeseries = df_timeseries.sort_values([\"country\", \"date\"])\n",
    "\n",
    "#  Split into countries\n",
    "df_timeseries_svk = df_timeseries[df_timeseries[\"country\"] == \"Slovakia\"].copy()\n",
    "df_timeseries_no = df_timeseries[df_timeseries[\"country\"] == \"Norway\"].copy()\n",
    "\n",
    "\n",
    "#  Calculate daily differences \n",
    "df_timeseries_svk[\"cases_number_diff\"] = df_timeseries_svk[\"cases_number\"].diff()\n",
    "df_timeseries_svk[\"deaths_number_diff\"] = df_timeseries_svk[\"deaths_number\"].diff()\n",
    "\n",
    "df_timeseries_no[\"cases_number_diff\"] = df_timeseries_no[\"cases_number\"].diff()\n",
    "df_timeseries_no[\"deaths_number_diff\"] = df_timeseries_no[\"deaths_number\"].diff()\n",
    "\n",
    "\n",
    "#Checking last values \n",
    "print(df_timeseries_svk.iloc[-3 :, :])\n",
    "print(df_timeseries_no.iloc[-5 :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e328af38-23ab-46c1-81df-83f6cb64eda4",
   "metadata": {},
   "source": [
    "#TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fadec1-c0e9-4d3f-b9a2-0f4ce689a229",
   "metadata": {},
   "source": [
    "Both columns, cases_number_diff and deaths_number_diff are correctly calculated in each, Slovakian and Norwegian dataset. I will visualize differences in cases:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4796c72f-0db9-4cdc-aed0-545768614c9f",
   "metadata": {},
   "source": [
    "## 8.2 Daily differences in incidence and mortality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd597d2c-e07e-4e3e-b869-dd0161691ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot for daily cases in Norway\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.lineplot(x = \"date\", y = \"cases_number_diff\", data = df_timeseries_no)\n",
    "plt.xticks(rotation = 45)\n",
    "plt.ylabel(\"Cases number daily difference\")\n",
    "plt.title(\"Daily differences in number of cases Norway\")\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "#Plot for daily cases in Slovakia\n",
    "sns.lineplot(x = \"date\", y = \"cases_number_diff\", data = df_timeseries_svk)\n",
    "plt.xticks(rotation = 45)\n",
    "plt.ylabel(\"Cases number daily difference\")\n",
    "plt.title(\"Daily differences in number of cases Slovakia\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33abe77-6d6d-4cbf-8a27-a055bb056288",
   "metadata": {},
   "outputs": [],
   "source": [
    "#days with maximum amount of cases\n",
    "print(df_timeseries_no[\"cases_number_diff\"].max())\n",
    "print(df_timeseries_svk[\"cases_number_diff\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661e08f2-70a8-4a64-b0be-3ec08c83cef1",
   "metadata": {},
   "source": [
    "For both Norway and Slovakia daily differemces are quite profound. Norway had one wave of covid in first quartile 2025, then daily cases stayed controlled until september when number of daily cases increased steadily. Slovakia held covid cases difference quite low until oktober with overall three waves til may 2021. Data are quite noisy but this graph shows some simmilarities. For example, September is a month when daily cases started to increase potentially due to beginning of school year. there have been reported more than 6000 daily cases in january in slovakia at its peak,whereas in norway maximum number of daily cases was under 1700. I will look at daily deaths as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2704149a-1542-4a75-b26e-027d0bf44414",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot for daily deaths in Norway\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.lineplot(x = \"date\", y = \"deaths_number_diff\", data = df_timeseries_no)\n",
    "plt.xticks(rotation = 45)\n",
    "plt.ylabel(\"Deathsnumber daily difference\")\n",
    "plt.title(\"Daily differences in number of deaths Norway\")\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "#Plot for daily deaths in Slovakia\n",
    "sns.lineplot(x = \"date\", y = \"deaths_number_diff\", data = df_timeseries_svk)\n",
    "plt.xticks(rotation = 45)\n",
    "plt.ylabel(\"Deaths number daily difference\")\n",
    "plt.title(\"Daily differences in number of deaths Slovakia\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34362b9d-8b27-4d0c-b120-ba74abb252e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#maximum amount of deaths\n",
    "print(df_timeseries_no[\"deaths_number_diff\"].max())\n",
    "print(df_timeseries_svk[\"deaths_number_diff\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ded88e-655f-4216-b43c-7f7e0395bf9b",
   "metadata": {},
   "source": [
    "The plots show clear differences between the two countries in terms of daily COVID-19 death counts.\n",
    "In Slovakia, daily deaths remained close to zero until October 2020, after which the country experienced sharp waves of increased mortality, peaking at nearly 200 deaths per day.\n",
    "In contrast, Norway had a more stable and moderate death rate throughout the same period, with daily deaths rarely exceeding 30.\n",
    "Given the noisy nature of daily reporting and possible inconsistencies in data collection, I will proceed by modeling the time series using weekly aggregates to better capture trends and reduce random variation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7d7cdf-80be-4696-b33d-b555cca6a6d6",
   "metadata": {},
   "source": [
    "## 8.2 Weekly differences in incidence and mortality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b2f505-d20f-4744-80bf-fbd004348c33",
   "metadata": {},
   "source": [
    "Now I will calculate weekly differences by truncating date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84261652-517b-4375-969f-8b9cbb50be64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating truncated weekly averages for Norway\n",
    "df_weekly_avg_norway = \"\"\"\n",
    "                             SELECT country,\n",
    "                             DATE_TRUNC('week', date) AS week_start,\n",
    "                             SUM(cases_number) AS crude_cases, \n",
    "                             AVG(cases_number_diff) AS weekly_avg_cases_diff,\n",
    "                             SUM(cases_number_diff) AS weekly_sum_cases_diff,\n",
    "                             SUM(deaths_number) AS crude_deaths,\n",
    "                             AVG(deaths_number_diff) AS weekly_avg_deaths_diff,\n",
    "                             SUM(deaths_number_diff) AS weekly_sum_deaths_diff\n",
    "                             FROM df_timeseries_no\n",
    "                             GROUP BY  week_start, country\n",
    "                             ORDER BY  week_start \"\"\"\n",
    "df_timeseries_no_weekly = duckdb.query(df_weekly_avg_norway).to_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ffbb30-a1c2-419d-b0b8-cf41e791117d",
   "metadata": {},
   "source": [
    "63 rows indicate 63 weeks covering entire dataset period from 2020-01-20 til 2021-04-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42d0030-dc59-475d-b2cc-b92dea750b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating truncated weekly averages for Slovakia\n",
    "df_weekly_avg_svk = \"\"\"      SELECT country,\n",
    "                             DATE_TRUNC('week', date) AS week_start,\n",
    "                             SUM(cases_number) AS crude_cases, \n",
    "                             AVG(cases_number_diff) AS weekly_avg_cases_diff,\n",
    "                             SUM(cases_number_diff) AS weekly_sum_cases_diff,\n",
    "                             SUM(deaths_number) AS crude_deaths,\n",
    "                             AVG(deaths_number_diff) AS weekly_avg_deaths_diff,\n",
    "                             SUM(deaths_number_diff) AS weekly_sum_deaths_diff\n",
    "                             FROM df_timeseries_svk\n",
    "                             GROUP BY  week_start, country\n",
    "                             ORDER BY  week_start \"\"\"\n",
    "\n",
    "df_timeseries_svk_weekly = duckdb.query(df_weekly_avg_svk).to_df()\n",
    "df_timeseries_svk_weekly \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5d0ed6-fb64-4aef-9575-8cda0dfcb765",
   "metadata": {},
   "source": [
    "Same for Slovakia, 63 rows indicate 63 weeks covering entire dataset period from 2020-01-20 til 2021-04-10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5292c39e-a7a7-4d2d-8ea0-e4d89872f56b",
   "metadata": {},
   "source": [
    "Now I will create copy of the datasets and concatenate them into both dataframe. This is neccessarry in order to plot them into one plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0293903-eaf4-4e20-bb25-2f01a89bd385",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekplot = pd.concat([df_timeseries_no_weekly ,df_timeseries_svk_weekly ], ignore_index = True)\n",
    "df_weekplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b5fa60-c443-417d-839d-4299a6b4d0b9",
   "metadata": {},
   "source": [
    "Dataset concatenated successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edae4d0b-b654-47e2-b672-1da9c168df10",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,5))\n",
    "sns.lineplot(x = \"week_start\", y = \"weekly_avg_cases_diff\", data = df_weekplot, hue = \"country\",marker = \"o\", markersize = 5 )\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.xlabel(\"period\")\n",
    "plt.ylabel(\"weekly sum in average cases\")\n",
    "plt.title(\"weekly difference in average cases by country\",fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9c659f-f8b1-4c32-90f1-f3f427df4793",
   "metadata": {},
   "source": [
    "Norway consistently maintained a significantly lower number of cases compared to Slovakia throughout the depicted period (early 2020 to mid-2021). Slovakia experienced a primary surge peaking near 3000 weekly cases around late 2020/early 2021, followed by another substantial increase in early 2021. In contrast, Norway's peaks were considerably smaller, reaching a maximum of approximately 900 weekly cases. Visually, Norway appears to have experienced four distinct waves of varying magnitudes, while Slovakia shows two prominent waves after an initial minor rise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f63752-22fc-4607-bb25-ddd79a9b00a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,5))\n",
    "sns.lineplot(x = \"week_start\", y = \"weekly_avg_deaths_diff\", data = df_weekplot, hue = \"country\",marker = \"o\", markersize = 5 )\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.xlabel(\"period\")\n",
    "plt.ylabel(\"weekly difference in average deaths\")\n",
    "plt.title(\"weekly difference in average deaths by country\",fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9781b1db-dc4a-4e46-b5c9-0addaefc29fe",
   "metadata": {},
   "source": [
    "A stark disparity in death rates between the two countries is evident. Slovakia experienced significantly higher weekly deaths, reaching peaks close to 100 deaths, particularly correlating with its major case surges observed in the previous figure. Conversely, Norway maintained consistently low death numbers, with weekly peaks rarely exceeding 10 deaths. This profound difference in fatalities highlights a substantial divergence in the pandemic's impact on mortality between Norway and Slovakia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7172cc-bd1e-41de-932c-977408df0680",
   "metadata": {},
   "source": [
    "Incidence rate\n",
    "to calculate incidence rate we have to have population of both countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19f9101-9bce-4956-8933-7532e368b761",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timeseries_no_weekly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a55798-5dc3-458a-8895-9705c286c742",
   "metadata": {},
   "source": [
    "# 9. Seasonal decomposition analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6963b04c-9d8a-4db8-9acc-5bf492bc44e6",
   "metadata": {},
   "source": [
    "Important part of time series analysis is  seasonal decomposition. Our data is however within an interval january 2020 til march 2021 with a very low amount of cases  til the september 2021. To investigate seasonal trends, I would have to have data for minimum period of two years. We can however see an amplitude of daily cases that Norway (circa 250 per day) in late march and april 2021) so I can access seasonality. Slovakia had almost zero cases during that period so I will not investigate seasonality in this country. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2d7d9e-d022-4fc2-bc21-ee7a19591c38",
   "metadata": {},
   "source": [
    "## 9.1 Seasonal decomposition of deaths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dbc924-5a17-4647-9132-78bfa2b9071b",
   "metadata": {},
   "source": [
    "First i will reset index in respective datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce168a7-74ef-4b84-90eb-a8f075cf1c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timeseries_no_weekly.set_index(\"week_start\", inplace = True)\n",
    "df_timeseries_no_weekly.index.freq = 'W-MON'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227b6c3b-3f24-49e4-8024-33e1b1881b06",
   "metadata": {},
   "source": [
    "I choose additive model because we have a lot of zero values in the dataset. As i expect almost no seasonality due to the data limitation, I will conduct seasonal decomposition of additive mode. This will allow me to include also data which contain 0 cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ed43d2-a24e-4584-a8a4-f65d4ded247d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition = seasonal_decompose(df_timeseries_no_weekly[\"crude_deaths\"], model = \"add\", period = 4)\n",
    "fig = decomposition.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ae72c4-4820-4ca6-a85d-b7c623073de3",
   "metadata": {},
   "source": [
    "I performed a seasonal decomposition of the weekly death differences to explore potential monthly seasonality (using a 2-week cycle). The trend component is stradily increasing before it drops last week\n",
    "\n",
    "The seasonal component shows a clear, although minor, repeating 4-week pattern ranging from approximately -2 to 2, suggesting low seasonal influence.\n",
    "\n",
    "The residuals appear randomly distributed, with tighter clustering around 0 . In additive model, residuals centered around 1 indicate a reasonably good model fit — suggesting that most systematic seasonal and trend patterns have been captured, leaving only irregular noise shown as randomly distributed residuals.\n",
    "\n",
    "Due to the limited data and questionable stability of the seasonal pattern, I will exclude seasonality from future time series models. This seasonal decomposition serves primarily to demonstrate Python coding skills and domain understanding, and will not be used for predictive modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69494055-a71f-4e35-b159-b14ac4e4b193",
   "metadata": {},
   "source": [
    "## 9.2 Auto correlation of deaths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7f72b7-3f4d-44cd-97e8-80aa322ba1f7",
   "metadata": {},
   "source": [
    "In this part I am going to correlate the values in our dataset with previous values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcc2ad7-be48-4ddc-b726-c68182e59f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(df_timeseries_no_weekly[\"crude_deaths\"], lags = 35)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e9b487-1306-4038-ba23-b9af0d026d93",
   "metadata": {},
   "source": [
    "\n",
    "The autocorrelation function (ACF) shows a strong and statistically significant positive autocorrelation up to lag 6, indicating that weekly deaths highly influenced by the previous week's deaths. Autocorrelation declines steadily and becomes negative in the week number 25 This suggests a cyclical reversal in the pattern of weekly death changes roughly 7. Overrall, this suggests short-term memory in the data (especially the first few weeks), along with possible medium-range cycles (7–20 weeks), but no long-term autocorrelation beyond ~35 weeks.) plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a59aca2-985e-4641-9c16-fceaed2636eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pacf(df_timeseries_no_weekly[\"crude_deaths\"], lags = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bac3276-6634-49d7-9366-d27bef143c38",
   "metadata": {},
   "source": [
    "The PACF plot shows a strong and statistically significant positive partial autocorrelation at lag 1, suggesting a clear autoregressive effect from the previous week's value.\n",
    "\n",
    "Lag 2 exhibits a small negative spike, but it falls within the confidence interval, meaning it's not statistically significant. Beyond lag 2, partial autocorrelations are all small and within bounds, indicating no substantial additional autoregressive structure.\n",
    "\n",
    "Based on this, I will start with a SARIMAX model using 1 or 2 non-seasonal autoregressive terms (p=1 or p=2) and compare their performance. The plot clearly supports at least one AR term; the second is included as a candidate for potential improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37910fc6-c6b5-4ca5-8ed2-a6925f6d332e",
   "metadata": {},
   "source": [
    "# 10. Data splitting into test and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e545d23-e70d-4648-bfc8-0a242cf0ad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training  data will have 51 weeks whereas test data will have 13 weeks:\n",
    "train_no_deaths, test_no_deaths = df_timeseries_no_weekly.iloc[: 52, -3], df_timeseries_no_weekly.iloc[52 :, -3]\n",
    "\n",
    "#Check\n",
    "print(train_no_deaths.head())\n",
    "print(test_no_deaths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eddcc6-56bc-44e3-8732-edafe1ebe561",
   "metadata": {},
   "source": [
    "# 11 Gradual exponential smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bd8b25-e0c4-4c58-ac61-c0634e7c9b9c",
   "metadata": {},
   "source": [
    "## 11.1 Simple exponential smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487d2741-c52f-4ff5-9b7e-86f7ec3ff855",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_no_cases_simple = ExponentialSmoothing(train_no_deaths, trend = None, seasonal = None).fit()\n",
    "forecast_no_cases = model_no_cases_simple.forecast(len(test_no_deaths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4b1749-72d5-46e2-9d58-c78e962291b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_no_deaths, label = \"Train\")\n",
    "plt.plot(test_no_deaths, label = \"Test\")\n",
    "plt.plot(forecast_no_cases, label  = \"Forecast\")\n",
    "plt.title(\"Train test and prediction with the simple exponential smoothing\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48a8647-d743-4401-8631-1512ec753797",
   "metadata": {},
   "source": [
    "Prediction line is flat indicating weighted average on the last point of test data. the model predicted cca 45 for each test data indicating the models best guess with weighted average of the last data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f3e5bb-3793-4afb-86f1-a98d5384af21",
   "metadata": {},
   "source": [
    "## 11.2 Double Exponential smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc5e2fa-07d2-4f57-8c49-ec842be62912",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_no_cases_double = ExponentialSmoothing(train_no_deaths, trend = \"add\", seasonal = None).fit()\n",
    "forecast_no_cases = model_no_cases_double.forecast(len(test_no_deaths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408ccd62-db92-45f9-99dd-bc99a0f82a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_no_deaths, label = \"Train\")\n",
    "plt.plot(test_no_deaths, label = \"Test\")\n",
    "plt.plot(forecast_no_cases, label  = \"Forecast\")\n",
    "plt.title(\"Train test and prediction with double exponential smoothing\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e78a06-1848-4d46-be2c-d08f0e5ff10a",
   "metadata": {},
   "source": [
    "The double exponential smoothing taking into consideration weighted averages and average trend shows straight line with circa 45 degree inclination. Inclination means that the model took into account average trend but ommited other parameters such as seasonality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66de7496-a447-4d67-b4c1-a2fd1ce99696",
   "metadata": {},
   "source": [
    "## 11.3 Forecasting With Holt-Winters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9c985d-7eca-4729-a115-e811e0df35ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feeding the model with the train data adjusting for trend, seasonality and number of periods\n",
    "model_no_cases_triple = ExponentialSmoothing(train_no_deaths, trend = \"add\", seasonal = \"add\", seasonal_periods =19).fit()\n",
    "\n",
    "#Forecasting and comparing the Holt-winters model predictions with actual test data\n",
    "forecast_no_cases_triple = model_no_cases_triple.forecast(len(test_no_deaths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60f8d56-e5e0-4222-a7b6-5f43e44624ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the plot to visualize the results\n",
    "plt.plot(train_no_deaths, label = \"Train\")\n",
    "plt.plot(test_no_deaths, label = \"Test\")\n",
    "plt.plot(forecast_no_cases_triple, label  = \"Forecast\")\n",
    "plt.title(\"Train test and prediction with triple exponential smoothing (Holt- Winters)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Key Performance Indicators:\n",
    "\n",
    "# Mean absolute error \n",
    "mae = mean_absolute_error(test_no_deaths, forecast_no_cases_triple)\n",
    "print(f\"The MAE is {mae:.2f} \")\n",
    "\n",
    "# Random mean squared error\n",
    "rmse = root_mean_squared_error(test_no_deaths, forecast_no_cases_triple)\n",
    "print(f\"The RMSE is {rmse:.2f} \")\n",
    "\n",
    "#Mean absolute percentage error\n",
    "mape = mean_absolute_percentage_error(test_no_deaths, forecast_no_cases_triple)\n",
    "print(f\"The mape is {100 * mape:.2f}% \")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0956f63c-a78d-47c3-8935-554602b6d845",
   "metadata": {},
   "source": [
    "With triple exponential smoothing which takes into consideration weighted averages, Trend and seasonality the forecast curve copies the shape of the test data more precisely, although it misses the predicted values significantly during the spike. I will check errors to see how well, or in our case, unwell our model performs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c240499-2102-4909-a3ea-3bb3d01dd5e8",
   "metadata": {},
   "source": [
    "Mean absolute error (MAE) of our model means it missed on average 294 deaths per week which is relatively low to overall number of deaths(more than 4000).\n",
    "The Random mean squared error(RMSE) is 327.32 which means that my Holt- winters forecast were a lot of deaths astray and those forecast errors were heavily penalized (RMSE > MAE). We can see that the forecast was quite off in predicting the number of weekly deaths. This is expected because the Holt-winter model assumes smooth trend and seasonal pattern. There is however a sudden collapse in number of deaths in the end of the dataset and it is something that Holt-winters can not account for as it is part of the train data.\n",
    "Mean absolute percentage error is  6.80%  meaning which suggest quite a good performance. I will try to improve model with sarimax with cases number as an exogenous variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c35e32-a4c5-400b-a362-f0ea6ffba34f",
   "metadata": {},
   "source": [
    "The main limitation of Holt-winters model is that it uses just historical data without any externalities such as vaccinations or public health measures or number of cases.To include other variables in the model we have to use models from ARIMA family. Sarima model with x as exogenous variable which helps us predict future cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2175769f-21cb-4e84-b27f-1ac2f41422a1",
   "metadata": {},
   "source": [
    "# 12 ARIMA and SARIMA modelels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c21f910-6dc5-428e-a311-436979f17c3e",
   "metadata": {},
   "source": [
    "## 12.1 Differencing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39957eff-adb6-4303-b2fc-ca5a981672e1",
   "metadata": {},
   "source": [
    "ARIMA Models\n",
    "In this part of the project, I will use ARIMA, SARIMA, and SARIMAX models.\n",
    "All these models assume the data is stationary, meaning the mean and variance are constant over time.\n",
    "There is no trend or seasonality.\n",
    "If the data is non-stationary, we apply differencing(substracting an observation from the previous one.\n",
    "\n",
    "To test for stationarity, we use the Augmented Dickey-Fuller (ADF) test:\n",
    "\n",
    "Null Hypothesis (H₀): The series is non-stationary.\n",
    "Alternative Hypothesis (H₁): The series is stationary.\n",
    "\n",
    "The following code is generated by chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e93cb7f-c78f-4023-be1f-8a3cf4ff036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = df_timeseries_no_weekly[\"crude_deaths\"].dropna()\n",
    "\n",
    "if len(series) >= 20:  # Minimum sample size\n",
    "    result = adfuller(series, autolag=\"AIC\")  # Optimal lag selection\n",
    "    \n",
    "    print(\"ADF Statistic:\", result[0])\n",
    "    print(\"p-value:\", result[1])\n",
    "    print(\"Critical Values:\", result[4])\n",
    "    \n",
    "    if result[1] <= 0.05:\n",
    "        print(\"✅ Stationary (reject H₀)\")\n",
    "    else:\n",
    "        print(\"❌ Non-stationary (fail to reject H₀)\")\n",
    "else:\n",
    "    print(\"⚠️ Insufficient data for ADF test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56af1fe6-b61e-4c58-8396-f88d0d682522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from the original cumulative series\n",
    "original_series = df_timeseries_no_weekly[\"crude_deaths\"].dropna()\n",
    "\n",
    "# First difference: new daily cases\n",
    "diff1 = original_series.diff().dropna()\n",
    "\n",
    "# Second difference: change in daily cases\n",
    "diff2 = diff1.diff().dropna()\n",
    "diff1\n",
    "# Plot all 3 for comparison\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(original_series, color='blue')\n",
    "plt.title(\"Original (Cumulative Cases)\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(diff1, color='green')\n",
    "plt.title(\"1st Difference (Daily New Cases)\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(diff2, color='orange')\n",
    "plt.title(\"2nd Difference (Δ Daily Cases)\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ADF test on second-differenced series\n",
    "result_2nd = adfuller(diff2)\n",
    "print(\"\\n🔁 ADF Test on 2nd Differenced Series\")\n",
    "print(\"ADF Test Statistic:\", result_2nd[0])\n",
    "print(\"p-value:\", result_2nd[1])\n",
    "print(\"Used lags:\", result_2nd[2])\n",
    "print(\"Observations:\", result_2nd[3])\n",
    "print(\"Critical Values:\")\n",
    "for key, value in result_2nd[4].items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "if result_2nd[1] <= 0.05:\n",
    "    print(\"\\n✅ Stationary after second differencing — ready for ARIMA modeling.\")\n",
    "else:\n",
    "    print(\"\\n❌ Still non-stationary — consider log transform or additional techniques.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161c2459-2a1e-476b-90a2-fe5317c752bf",
   "metadata": {},
   "source": [
    "Our deaths data needed two differencing (d = 2) s to become stationary and ready to be used for ARIMA models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ee43e5-231f-4a19-a537-e6fa4a2661c0",
   "metadata": {},
   "source": [
    "##  12.2 Arima model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc094b3-82be-45ef-ac93-a0b7996831ca",
   "metadata": {},
   "source": [
    " In this subsection I will create an AutoRegressive Integrated Moving Average (ARIMA) model and tracing how the model is selected.\n",
    "The ARIMA model consists of three main components:\n",
    "\n",
    "- **Autoregressive (AR):** This component captures the relationship between the current value and its own past values. For example, how the number of deaths in the current week may depend on deaths in previous weeks. The optimal number of AR terms can often be determined by examining the **partial autocorrelation function (PACF)** plot.\n",
    "\n",
    "- **Integrated (I):** This refers to the number of times the data needs to be differenced to become stationary — i.e., to remove trends or seasonality. In this case, the data became stationary after two differencing steps, but using only one may be sufficient depending on model performance. Differencing can reduce accuracy if overused, so a trade-off is necessary.\n",
    "\n",
    "- **Moving Average (MA):** This component models the relationship between the current value and past **forecast errors** (residuals). Unlike AR, which looks at actual past values, MA focuses on how the errors in prediction contribute to the current observation. The **autocorrelation function (ACF)** plot helps determine the appropriate number of MA terms.\n",
    "\n",
    "The model was fitted using the `auto_arima()` function, which automates the process of selecting the best combination of parameters  (p, d, q). based on criteria such as the Akaike Information Criterion (AIC):\n",
    "I will suppress warnings first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d1b075-cd21-4b1e-a968-bf6b1793f093",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3ecf5f-3ded-4f91-bd5d-8c88bdb121fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_arima = auto_arima(train_no_deaths, trace = True)\n",
    "model_arima.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dbd082-c2fc-4d15-94ee-664b6e264b4b",
   "metadata": {},
   "source": [
    "\n",
    "The best model uses 2 autoregressive lags on the once differenced data, and no moving average terms. The differencing order of 1 was chosen despite ADF’s suggestion, likely because it gave a better trade-off between stationarity and model fit.\n",
    "ar. L1 1.3630 (CI 1.182 : 1.544) and ar.L2\t-0.5456 (CI -0.736\t-0.355) mean that there is a strong and significant positive effects from the first lag and significant  negative effects from the second lag respectively.Ljung-Box (L1) means there is no autocorrelation left in residuals and model captured all important patterns. The biggest concern is metric Jarque-Bera (JB): where p = 0.00 and we reject the null hypothesis that residuals in our model is normally distributed. I will  try to use logarithmic transformation to optimize the residuals later on. Before however, I will try to plot the training test and prediction values and calculate mean absolute error(mae), root squared squared  error (rmse), and mean absolute percentage error(mape) of the current model.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb09229-cb77-470c-b958-b44ace0f34e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_arima = model_arima.predict(n_periods = len(test_no_deaths))\n",
    "predictions_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75538e01-077f-4505-af63-47aeb3aad59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the plot to visualize the results\n",
    "plt.plot(train_no_deaths, label = \"Train\")\n",
    "plt.plot(test_no_deaths, label = \"Test\")\n",
    "plt.plot(predictions_arima, label  = \"Forecast\")\n",
    "plt.title(\"Train test and prediction with ARIMA\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Error measures\n",
    "mae = mean_absolute_error(test_no_deaths, predictions_arima)\n",
    "print(f\"The MAE is {mae:.2f} \")\n",
    "rmse = root_mean_squared_error(test_no_deaths, predictions_arima)\n",
    "print(f\"The RMSE is {rmse:.2f} \")\n",
    "mape = mean_absolute_percentage_error(test_no_deaths, predictions_arima)\n",
    "print(f\"The mape is {100 * mape:.2f}% \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7504dd-60ec-41fd-b6c8-4cf0fbc00590",
   "metadata": {},
   "source": [
    "Our ARIMA model(2, 1, 0) has worse performance(MAE 386.52, RMSE 422.96) than  than Winter-holt model (MAE 294.37, RMSE 327.32, mape 6.80%). I will try sarimax but i assume it will have insubstantial effects on predictions due to almost non-existent seasonality and short data span. but before that I will try to transform the data to normalize distribution of the residuals in ARIMA model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d4a321-95db-43fb-a16a-8e864a36f4fc",
   "metadata": {},
   "source": [
    "The biggest concern is the Jarque-Bera (JB) test. The p-value of 0.00 means that we reject the null hypothesis that our residuals are normally distributed. This non-normal distribution can lead to inaccurate confidence intervals and forecasts due to the high variance of the residuals. To address this, I will try to apply a logarithmic transformation to the data. This transformation helps to stabilize the variance of the residuals and make their distribution more normal, which increases the reliability of the model's predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e62f3be-0b35-4d3c-8917-841bc59bdd64",
   "metadata": {},
   "source": [
    "### Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04217f50-3d1b-404c-a9ac-73d9270843d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating model residuals and fited values\n",
    "residuals_arima = model_arima.arima_res_.resid\n",
    "fitted_values_arima = model_arima.arima_res_.fittedvalues\n",
    "\n",
    "#Creating ACF plot of the residuals\n",
    "plot_acf(residuals_arima, lags = 35)\n",
    "plt.show()\n",
    "\n",
    "#Creating PACF plot of the residuals\n",
    "plot_pacf(residuals_arima, lags = 24)\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(fitted_values_arima, residuals_arima)\n",
    "plt.title('Residuals vs. Fitted Values')\n",
    "plt.xlabel('Fitted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25528071-a368-4e65-ab13-37c683c5d518",
   "metadata": {},
   "source": [
    "The ACF and PACF plots confirm that there is no significant autocorrelation or partial autocorrelation in the residuals. Furthermore, the residuals versus fitted values plot shows that the residuals are randomly scattered around zero with constant variance, indicating homoscedasticity.\n",
    "This suggests that the model has effectively captured the underlying patterns in the data, leaving no apparent structure in the residuals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9d4abe-05d5-4f54-8d42-5a95530e9f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating and print the original kurtosis\n",
    "original_kurtosis_arima = kurtosis(residuals_arima)\n",
    "print(f\"The original kurtosis of the residuals is: {original_kurtosis_arima:.2f}\")\n",
    "\n",
    "# Create a histogram of the residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "residuals_arima .hist(bins=30, density=True)\n",
    "plt.title('Histogram of Residuals for ARIMA(2,1,0) Model')\n",
    "plt.xlabel('Residuals of model without exogenous variable')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# Adding a normal distribution curve for comparison\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, residuals_arima .mean(), residuals_arima .std())\n",
    "plt.plot(x, p, 'k', label='Normal Distribution')\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sm.qqplot(residuals_arima, line='45', fit=True)\n",
    "plt.title('Q-Q Plot of Residuals')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a28c55-278f-4f05-a092-0b962057d9ac",
   "metadata": {},
   "source": [
    "Histogram shows the distribution of residuals which seems normal with slight positive skew. The variance of residuals  ranging from -100 to 150). Q-Q plot shows that residuals are oscilating with slight deviation around the red line representing normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e678f7c9-19ad-4292-8e7d-6df364e4e1d2",
   "metadata": {},
   "source": [
    "## 12.2.1 Log-transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3941ef4-755c-4722-b5e4-2c3c7c00ca6a",
   "metadata": {},
   "source": [
    "To address non-normality and in the residuals, I applied a logarithmic transformation to the weekly mortality counts before training the ARIMA model. Specifically, I modeled the log-transformed data, which often stabilizes variance and improves the model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c5e278-d9cd-4330-ba35-85063613d44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming data to logaritmic scale\n",
    "y_transformed = np.log(train_no_deaths +1).to_frame()\n",
    "#transformed training  data will have 51 weeks whereas test data will have 13 weeks:\n",
    "train_no_deaths_log, test_no_deaths_log = y_transformed.iloc[7:52], y_transformed.iloc[52 :]\n",
    "y_transformed.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26605d17-f678-4968-982d-de7308a802fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Letting auto_arima choosing the best model\n",
    "model_arima_log = auto_arima(train_no_deaths_log, trace = True)\n",
    "model_arima_log.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f789529-2722-43cf-9770-76ef1fb7e2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting original dataset  with model_arima_log \n",
    "predictions_log = model_arima_log.predict(n_periods = len(test_no_deaths))\n",
    "# transforming predictions in logharitmic scale back to the exponential scale.\n",
    "predictions_original_scale = np.exp(predictions_log) - 1\n",
    "predictions_original_scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece79c3b-e5de-4ee5-9b79-40b41d4ab038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the plot to visualize the log-transformed ARIMA results\n",
    "plt.plot(train_no_deaths, label = \"Train\")\n",
    "plt.plot(test_no_deaths, label = \"Test\")\n",
    "plt.plot(predictions_original_scale, label = \"Forecast\")\n",
    "plt.title(\"Train, Test, and Prediction (Inverse-Transformed Forecast)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ee5e6d-f592-4226-a9ee-025c4a7bb9b4",
   "metadata": {},
   "source": [
    "In our log-transformed arima model (5,2,0) Jarque-Bera  worsened from 11.94 to 117 (p = 0.00) this suggests non-normal distribution of  residuals.\n",
    "Ljung-Box p = 0.060 indicates that we failed to reject the hypothesis that residuals are correlated.log transformation attempt worsened normality (JB=45.87 vs 11.94). In addition visual evaulation suggests that this model is unacceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e9b3d2-8e4e-4f02-bd37-82cff211b49d",
   "metadata": {},
   "source": [
    "# 12.3 SARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a91cb5d-e4aa-46ad-9e55-6537be554f45",
   "metadata": {},
   "source": [
    "Sarima model are bbasically arima models wiith seasonal component incorporated in ARIMA it so I expect the auto_arima function will have seasonal component  (P,Q,D = 0,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a22003-5819-43d9-9e37-3cd27718cbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Sarima model\n",
    "model_sarima_auto = auto_arima(train_no_deaths, trace = True, m = 20, seasonal = True)\n",
    "model_sarima_auto.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73effe7-32d3-4fdc-9a95-28c2e3143e0b",
   "metadata": {},
   "source": [
    "I tried several m periods and the best model is same as arima model SARIMA(2, 1, 0) (0,0,0) m= 20 with the same low probability of autocorrelation in residuals  Ljung-Box (L1) (Q):\t0.18 (p = 0.67), and high probability(p = 0.00) of non-normal distribution of residuals (Jarque-Bera = 11.94)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2e7bac-ad34-4db0-a5c2-ef4b747e8442",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sarima = model_sarima_auto.predict(n_periods = len(test_no_deaths), seasonal = 20)\n",
    "predictions_sarima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb900bcc-efb8-4361-8f83-a3df10717323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the plot to visualize the results of SARIMA\n",
    "plt.plot(train_no_deaths, label = \"Train\")\n",
    "plt.plot(test_no_deaths, label = \"Test\")\n",
    "plt.plot(predictions_sarima, label  = \"Forecast\")\n",
    "plt.title(\"Train test and prediction with SARIMA\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Error measures\n",
    "mae = mean_absolute_error(test_no_deaths, predictions_sarima)\n",
    "print(f\"The MAE is {mae:.2f} \")\n",
    "rmse = root_mean_squared_error(test_no_deaths, predictions_sarima)\n",
    "print(f\"The RMSE is {rmse:.2f} \")\n",
    "mape = mean_absolute_percentage_error(test_no_deaths, predictions_sarima)\n",
    "print(f\"The mape is {100 * mape:.2f}% \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb98311-a8e3-4e46-959f-c4030f1407be",
   "metadata": {},
   "source": [
    "MAE RMSE MAPE and Prediction curve are the same as on ARIMA model on untransformed data. The Sarima model is basically ARIMA in this case.\n",
    "While both ARIMA and SARIMA models produced acceptable fits, their limited inclusion of external predictors reduces their value for proactive public health decision-making. In outbreak response, models enriched with epidemiological drivers (e.g., mobility, vaccination rates) are often more actionable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e97c12d-aab6-4977-8524-dd6a825a1dc9",
   "metadata": {},
   "source": [
    "# 13. ARIMAX model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e00ee3-91d1-4270-b9f8-b48f119ce7ef",
   "metadata": {},
   "source": [
    "Arimax model is ARIMA model containing one or more exogenous variables which can help us predict the future. I hypothesise that number of cases is a significant predictor of number of deaths. I decided use number of cases as an exogenous variable in time series modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21108754-2fa0-4d8c-9939-2e676f570544",
   "metadata": {},
   "source": [
    "First I have to create training and test dataset of exogenous variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbf24dd-e161-4098-9921-e79a5c31c5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training  data will have 51 weeks whereas test data will have 13 weeks:\n",
    "train_exog, test_exog = df_timeseries_no_weekly.iloc[: 52, [1]], df_timeseries_no_weekly.iloc[52 :, [1]]\n",
    "assert train_no_deaths.index.equals(train_exog.index)\n",
    "#Check\n",
    "print(train_exog.head())\n",
    "print(test_exog)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dd1f1d-fbe0-4c5c-be46-df7849158379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling Arimax model\n",
    "model_arimax = auto_arima(train_no_deaths, X = train_exog, trace = True, m = 20, seasonal = True)\n",
    "model_sarima_auto.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e66833-cb4b-4cf1-8974-51b86ae4d078",
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing out predictions\n",
    "predictions_arimax = model_sarima_auto.predict(n_periods = len(test_no_deaths),X = test_exog, seasonal = 20)\n",
    "predictions_arimax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207b2b1b-d7e9-471a-9cb1-967fc27dcd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the plot to visualize the results of ARIMAX\n",
    "plt.plot(train_no_deaths, label = \"Train\")\n",
    "plt.plot(test_no_deaths, label = \"Test\")\n",
    "plt.plot(predictions_arimax, label  = \"Forecast\")\n",
    "plt.title(\"Train test and prediction with ARIMAX\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Error measures\n",
    "mae = mean_absolute_error(test_no_deaths, predictions_arimax)\n",
    "print(f\"The MAE is {mae:.2f} \")\n",
    "rmse = root_mean_squared_error(test_no_deaths, predictions_arimax)\n",
    "print(f\"The RMSE is {rmse:.2f} \")\n",
    "mape = mean_absolute_percentage_error(test_no_deaths, predictions_arimax)\n",
    "print(f\"The mape is {100 * mape:.2f}% \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e12f35b-9a12-4f7c-a551-8e60d8e2cb36",
   "metadata": {},
   "source": [
    "Exogenous variable had no effect number of deaths has no predictive significance and metrics for ARIMA and ARIMAX are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa6e2a0-de87-4e9c-b875-79fb22d07a27",
   "metadata": {},
   "source": [
    "In Arimax model the log- likelihood insubstantial log likelihood indicates better marginally better model fit. On the other hand ARIMA model without exogenous variable has slightly better balance between model complexity and model fit(AIC 532.1 vs 533.3 for ARIMA and ARIMAX respectively). The crude_cases p = 2.48 indicates that we have failed to reject the null hypothesis that there is no relationship between cases and deaths this variable. Ljung Box 0.18(p=0.67) indicates no residual autocorrelation left both ARIMA and ARIMAX models. The non-normality of residuals remains(Jarque-Bera p = 0.00) remains a major issue in both models. In conclusion, Even though ARIMAX was better in predicting deaths, insignificant exogenous variable made the ARIMAX model overly complex resulting in overall worse performance of this time series model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b5fbef-4334-446c-be34-66cb512cbdc5",
   "metadata": {},
   "source": [
    "Even though ARIMAX was better in predicting deaths, insignificant exogenous variable made the ARIMAX model overly complex resulting in overall worse performance of this time series model. \n",
    "\n",
    "Previous model didn´t capture relationship between cases and deaths well. There might be several reasons for it. One can be a very good performance of the norwegian healthcare system, another is that there a certain period between being diagnosed to dying ranging from one week to several weeks. Therefore, I am going to use lagged weekly cases up to three weeks as an exogenous variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18404351-fab0-4b4a-9467-43f844b32b9d",
   "metadata": {},
   "source": [
    "# 14 ARIMAX with lagged cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c344f4-ded0-4eb6-8999-2ffe91ec83e5",
   "metadata": {},
   "source": [
    "In this section, I will construct exogenous variables representing lagged case counts to capture the temporal relationship between incident cases and subsequent mortality. Specifically, cases_lag_1 will represent the number of reported cases with a one-week lag relative to deaths, cases_lag_2 a two-week lag, and cases_lag_3 a three-week lag. These lagged variables are intended to account for the delay between infection detection and death, thereby improving the model’s ability to predict future COVID-19–related mortality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a964f8c2-0da4-4e17-b990-265bd9fe70d1",
   "metadata": {},
   "source": [
    "First i will  create a new dataset with lagged cases and afterwards i will gradually include one two and three week lags for each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cc6ea9-010d-40f4-9016-0b3e11c3d5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new dataframe with lags\n",
    "df_lags = df_timeseries_no_weekly[[\"crude_deaths\", \"crude_cases\"]].copy()\n",
    "\n",
    "#Creating one, two and three-weeks lags\n",
    "df_lags[\"cases_lag1\"] = df_lags[\"crude_cases\"].shift(1)\n",
    "df_lags[\"cases_lag2\"] = df_lags[\"crude_cases\"].shift(2)\n",
    "df_lags[\"cases_lag3\"] = df_lags[\"crude_cases\"].shift(3)\n",
    "\n",
    "\n",
    "#Deleting first where calculating lags was not possible\n",
    "df_lags.dropna(inplace = True)\n",
    "df_lags.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942c4448-3295-453f-b381-90b93a71a889",
   "metadata": {},
   "source": [
    "Three rows were deleted from original dataset. This corresponds to the first three weeks where lags couldn´t be substracted because we missed information on three weeks before the first week in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8c98ec-e657-4ca6-8b6d-caacdea4060d",
   "metadata": {},
   "source": [
    "Now I will split the data into 48 and 13 weeks into training and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6828d283-9dfa-41f8-b56d-791a7a94bbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spliting the data into 48 and 13 weeks into training and testing dataset with one-week case-deaths delay as exogenous variable.\n",
    "train_lagged_y, test_lagged_y = df_lags.iloc[: 48, [0]], df_lags.iloc[48 :, [0]]\n",
    "train_exog, test_exog = df_lags.iloc[: 48, [2]] , df_lags.iloc[48 :, [2]]\n",
    "\n",
    "#Validating the split\n",
    "print(\"Is the length of training dataset and training x variable tha same?\\n\", len(train_lagged_y) == len(train_exog), \"\\nIs the length of test dataset and test x variable tha same?\\n\", len(test_lagged_y) == len(test_exog))\n",
    "if (train_lagged_y.index != train_exog.index).any() | (test_lagged_y.index != test_exog.index).any():\n",
    "    #print (train_lagged_y.index == train_exog.index)\n",
    "    print(\"Spliting went wrong\")\n",
    "else:\n",
    "    print(\"Spliting successful, indeces and length of the training dataset are the same,  proceed to modelling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ab314b-4d42-428d-a850-1cb5fb3ffc45",
   "metadata": {},
   "source": [
    "## 14.1 Arimax with one-week lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d80db96-dd9b-4708-bf69-b6193e6dcc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the ARIMAX lag_1 model\n",
    "model_arimax1= auto_arima(train_lagged_y, X = train_exog, trace = True, supress_warnings = True)\n",
    "model_arimax1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee02c787-d99b-4364-8eba-bc50f8401c58",
   "metadata": {},
   "source": [
    "Including cases_lag1 (cases from one week prior) improved model fit and interpretability. The exogenous variable was highly significant (p < 0.001), suggesting a strong lagged association between COVID-19 cases and deaths one week later. The model showed a substantially lower AIC (504.6 vs. 533.3) and better residual properties. Although residuals were still not perfectly normal (JB p = 0.02), they improved considerably over the previous model. No autocorrelation remained, confirming a well-specified model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8d7658-4b96-404a-bfdd-c57b6a8c3f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forecasting the number of deaths\n",
    "predictions_lagged1 = model_arimax1.predict(n_periods = len(test_lagged_y),  X = test_exog)\n",
    "predictions_lagged1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78232b4d-ed5a-43d5-b838-25f984d2a3ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating the plot to visualize the results of ARIMAX lag1 model\n",
    "plt.plot(train_lagged_y, label = \"Train\")\n",
    "plt.plot(test_lagged_y, label = \"Test\")\n",
    "plt.plot(predictions_lagged1, label  = \"Forecast\")\n",
    "plt.title(\"Train test and prediction with ARIMAX one-week incidence-mortality lag\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Error metrics\n",
    "mae = mean_absolute_error(test_lagged_y, predictions_lagged1)\n",
    "print(f\"The MAE is {mae:.2f} \")\n",
    "rmse = root_mean_squared_error(test_lagged_y, predictions_lagged1)\n",
    "print(f\"The RMSE is {rmse:.2f} \")\n",
    "mape = mean_absolute_percentage_error(test_lagged_y, predictions_lagged1)\n",
    "print(f\"The mape is {100 * mape:.2f}% \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7be7994-75dc-4f75-b4f7-07a06042b12e",
   "metadata": {},
   "source": [
    "\n",
    "The model_arimax1 with one-week period betwen case- deaths has worse mae 373(291 ARIMA), rmse 657(349 ARIMA) and mape 8.59%(6.85% ARIMA) compared to ARIMA. The rmse is very high presumably because the big prediction miss in the last data point. Despite the higher RMSE, this model is still preferable over ARIMA due to lower AIC/BIC and significant predictor as an exogenous variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3b42c5-2f48-40b0-b068-c6f7baef3d67",
   "metadata": {},
   "source": [
    "### Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0a825f-3cbb-46c7-b4e1-94bc0ac9bac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the residuals\n",
    "residuals_lag1 = model_arimax1.resid()\n",
    "print(residuals_lag1)\n",
    "\n",
    "print(\"Average without the first observation is \",np.average(np.absolute(residuals_lag1[1:])))\n",
    "print(\"Average without the first observation is \", np.average(np.absolute(residuals_lag1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1650750f-2a74-44dd-8352-bde43674fd57",
   "metadata": {},
   "source": [
    "\n",
    "The first prediction of the series was the worst ( miss 811) and excluding this observation the the model missed on average 26 deaths per week. The average absolute error doubled after including the first forecast(-811)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2f441e-da90-4d88-b439-14453d1f8870",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_acf(residuals_lag1, lags = 35)\n",
    "plt.show()\n",
    "plot_pacf(residuals_lag1, lags = 24)\n",
    "plt.show()\n",
    "fitted_values_lag1 = model_arimax1.fittedvalues()\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(fitted_values_lag1, residuals_lag1)\n",
    "plt.title('Residuals vs. Fitted Values')\n",
    "plt.xlabel('Fitted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45602a3a-d81a-43ef-8cd9-51a484a0a500",
   "metadata": {},
   "source": [
    "ACF and PACF plot show that there is no significant autocorrelation and partial autocorrelation observed in residuals. Residuals vs fitted values shows that residuals are clustered around zero with no visible pattern indicating homoskedasticity with constant variance and no visible pattern (autocorrelation left)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb014bb9-0088-426d-9c92-05b03ad811bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a histogram of the residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "residuals_lag1.hist(bins=30, density=True)\n",
    "plt.title('Histogram of Residuals for ARIMAX one-week cases-deaths lag Model')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# Add a normal distribution curve for comparison\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, residuals_lag1.mean(), residuals_lag1.std())\n",
    "plt.plot(x, p, 'k', linewidth=2, label='Normal Distribution')\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print the kurtosis\n",
    "kurtosis_value = kurtosis(residuals_lag1)\n",
    "print(f\"The kurtosis of the residuals is: {kurtosis_value:.2f}\")\n",
    "\n",
    "#plot the figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "sm.qqplot(residuals_lag1, line='45', fit=True)\n",
    "plt.title('Q-Q Plot of Residuals')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4519eacb-a2d0-41fe-8a62-62abf74a7285",
   "metadata": {},
   "source": [
    "The histogram is negatively skewed towards lower values indicating big variance of negative residuals due to outliars. The Q-Q plot shows that Q-Q do deviate from red line suggests their non-normal distribution with one data-point near -2 on X axis indicating outlyer. I will try to exclude the first observation to see whether skewness changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af37be5f-9e7b-4e8b-ba5b-9ce4f7bbd857",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate and print the original kurtosis\n",
    "original_kurtosis = kurtosis(residuals_lag1)\n",
    "print(f\"The original kurtosis of the residuals is: {original_kurtosis:.2f}\")\n",
    "\n",
    "# Calculate and print the kurtosis after dropping the first week's outlier\n",
    "residuals_without_outlier = residuals_lag1.iloc[1:]\n",
    "new_kurtosis = kurtosis(residuals_without_outlier)\n",
    "print(f\"The kurtosis without the first week's outlier is: {new_kurtosis:.2f}\")\n",
    "\n",
    "# Create a histogram of the residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "residuals_without_outlier.hist(bins=30, density=True)\n",
    "plt.title('Histogram of Residuals for ARIMAX lag1 Model')\n",
    "plt.xlabel('Residuals of model with one-week cases-deaths lag')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# Add a normal distribution curve for comparison\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, residuals_without_outlier.mean(), residuals_without_outlier.std())\n",
    "plt.plot(x, p, 'k', linewidth=2, label='Normal Distribution')\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print the kurtosis\n",
    "kurtosis_value = kurtosis(residuals_without_outlier)\n",
    "print(f\"The kurtosis of the residuals is: {kurtosis_value:.2f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sm.qqplot(residuals_without_outlier, line='45', fit=True)\n",
    "plt.title('Q-Q Plot of Residuals')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857bf02f-ad4b-4e13-a9c8-c62f0b930344",
   "metadata": {},
   "source": [
    "The analysis of normality has shown that kurtosis without first residual of the model(-811) is 2.34. Figures indicate there is no skew(2.34) and variance of the residuals is smaller. Lastly, Q-Q plot of residuals shows much better overall shape of distribution where datapoints copy the red line indicating normal distribution. This means that failiure of rejection of null hypothesis that the residuals have normal distribution in JB test(0.02) was heavily influenced by single explainable event. Specifically, the first data point where number of cases was zero but our ARIMAX model predicted value 811. After the event, the model quickly adjusted itself and the second observation had bellow average error indicating  in-place first-lag autocorrelation component. The big error lead to higher MAE,MAPE,RMSE variance skewness and kurtosis. Without the first observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6001c91-08a3-4363-a105-d7dcd8a9e9b3",
   "metadata": {},
   "source": [
    "## 14.2 Arimax with two-week lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f524cd56-ef7e-4f2f-a311-9f20ddd3220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing exogenous variables with two-weeks case-death predictions\n",
    "train_exog, test_exog = df_lags.iloc[: 48, [3]] , df_lags.iloc[48 :, [3]]\n",
    "\n",
    "model_arimax2= auto_arima(train_lagged_y, X = train_exog, trace = True, supress_warnings = True)\n",
    "model_arimax2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba6ec6b-b168-48b2-9d7f-af1f4cf875b0",
   "metadata": {},
   "source": [
    "Function auto_arima selected an ARIMA(2,0,1) model without an intercept, indicating a stationary series influenced by two autoregressive terms and one moving average term. The exogenous variable cases_lag2  had a significant positive association with deaths (coef = 0.0096, p < 0.001). The AR terms (AR1 = 1.74, AR2 = -0.75) were both highly significant, suggesting strong temporal dependence with a corrective influence from two weeks ago. The MA(1) term was also significant (coef = 0.34), reflecting some impact of recent forecast errors.\n",
    "Diagnostic tests showed no residual autocorrelation (Ljung-Box p = 0.99) and no strong evidence of heteroskedasticity (p = 0.08). However, residuals still did not appear normally distributed (JB p = 0.00), with moderate skew (0.55) and elevated kurtosis (6.38), indicating heavy tails.n Conclusively, this is the strongest model model with the lowest AIC/BIC(502/511 respectively) and significant exogenous variable as a predictor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bc94f6-82e7-475b-8a82-5e35e6169846",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lagged2 = model_arimax2.predict(n_periods = len(test_lagged_y),  X = test_exog)\n",
    "predictions_lagged2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742a91a7-9748-4034-bbb6-bcd650d739b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the plot to visualize the results of Arimax lag 2\n",
    "plt.plot(train_lagged_y, label = \"Train\")\n",
    "plt.plot(test_lagged_y, label = \"Test\")\n",
    "plt.plot(predictions_lagged2, label  = \"Forecast\")\n",
    "plt.title(\"Train test and prediction with ARIMAX two-week incidence-mortality lag\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Error metrics\n",
    "mae = mean_absolute_error(test_lagged_y, predictions_lagged2)\n",
    "print(f\"The MAE is {mae:.2f} \")\n",
    "rmse = root_mean_squared_error(test_lagged_y, predictions_lagged2)\n",
    "print(f\"The RMSE is {rmse:.2f} \")\n",
    "mape = mean_absolute_percentage_error(test_lagged_y, predictions_lagged2)\n",
    "print(f\"The mape is {100 * mape:.2f}% \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae75f39d-879c-4f27-86f4-89f7f10b3aba",
   "metadata": {},
   "source": [
    "Even though the diagnostics metrics(AIB/BIC) of this model(two-week case-death period as exogenous variable) were the best of all included models, performance metrics  mae 508.12, 7 RMSE 57.72 and mape 11.77% imply lower accuracy. Particularly RMSE 757 means that model failed to predict sudden collapse in number of cases in the last data period. Even though there is slightly better AIC(502) compared to the previous model, the error metrics AE RMSE and MAPE much higher. Lower AIC and higher error metrics in this model  imply a slightly better balance in complexity and a worse performance on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daf0a88-ca2e-43de-a123-03fa02c890fb",
   "metadata": {},
   "source": [
    "## 14.2 Arimax with three-week lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e61e06-6890-46e2-871f-3b7b9d5fa7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing exogenous variables with two-weeks case-death predictions\n",
    "train_exog, test_exog = df_lags.iloc[: 48, [4]] , df_lags.iloc[48 :, [4]]\n",
    "\n",
    "\n",
    "model_arimax3= auto_arima(train_lagged_y, X = train_exog, trace = True, supress_warnings = True)\n",
    "model_arimax3.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cdc23e-11a4-4e0a-951b-a8da23f1110e",
   "metadata": {},
   "source": [
    "Function auto_arima selected an ARIMA(3,0,0) model without an intercept, indicating a  series with zero differencing, influenced by three autoregressive terms. The exogenous variable cases_lag3, representing the number of cases three weeks prior, showed a strong positive association with deaths (coef = 0.0107, p < 0.001).\n",
    "The autoregressive terms (AR1 = 2.12, AR2 = -1.47, AR3 = 0.33) were all statistically significant, indicating a dynamic relationship with the past three weeks of the outcome, including alternating patterns of reinforcement and correction.\n",
    "\n",
    "Autocorellation left in residuals is unlikely (Ljung-Box p = 0.87), and variance appeared mostly constant, H-test p = 0.02).However there is small positive skewness(0.64) with high variance(5.09)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133e39a6-f09f-4bce-9fe1-b5dfa1534eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lagged3 = model_arimax3.predict(n_periods = len(test_lagged_y),  X = test_exog)\n",
    "predictions_lagged3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc64242c-1e9a-4372-bf4c-f06e41f968f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 14.2 Arimax with two-week lag\n",
    "plt.plot(train_lagged_y, label = \"Train\")\n",
    "plt.plot(test_lagged_y, label = \"Test\")\n",
    "plt.plot(predictions_lagged3, label  = \"Forecast\")\n",
    "plt.title(\"Train test and prediction with ARIMAX three-week incidence-mortality lag\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Error metrics\n",
    "mae = mean_absolute_error(test_lagged_y, predictions_lagged3)\n",
    "print(f\"The MAE is {mae:.2f} \")\n",
    "rmse = root_mean_squared_error(test_lagged_y, predictions_lagged3)\n",
    "print(f\"The RMSE is {rmse:.2f} \")\n",
    "mape = mean_absolute_percentage_error(test_lagged_y, predictions_lagged3)\n",
    "print(f\"The mape is {100 * mape:.2f}% \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473b3e6f-215a-483e-a321-c47f060d2612",
   "metadata": {},
   "source": [
    "The three-weeks period between death-case variables further exacerbated MAE RMSE and mape. Particularly RMSE 926  indicates big misses in forecasts which makes this model not suitable despite the best AIC/BIC scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01021e24-4115-446e-8d00-7ebd445be01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating model residuals and fited values\n",
    "residuals_arimax_lag3 = model_arimax3.arima_res_.resid\n",
    "fitted_values_arimax_lag3 = model_arimax3.arima_res_.fittedvalues\n",
    "\n",
    "#Creating ACF plot of the residuals\n",
    "plot_acf(residuals_arimax_lag3, lags = 35)\n",
    "plt.show()\n",
    "\n",
    "#Creating PACF plot of the residuals\n",
    "plot_pacf(residuals_arimax_lag3, lags = 24)\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(fitted_values_arimax_lag3, residuals_arimax_lag3)\n",
    "plt.title('Residuals vs. Fitted Values')\n",
    "plt.xlabel('Fitted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca91cf3f-d073-4fdb-b516-b5452c366989",
   "metadata": {},
   "source": [
    "The residuals versus fitted values plot shows the residuals are randomly scattered around zero with a constant variance, suggesting the model's errors are homoscedastic. This, along with the lack of significant autocorrelation shown in the ACF and PACF plots (as you've noted), indicates that the model has effectively captured the underlying patterns in the data, leaving no discernible structure in the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b809607-5e12-42f8-90ec-a091a1a46002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print the original kurtosis\n",
    "\n",
    "# Create a histogram of the residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "residuals_arimax_lag3.hist(bins=30, density=True)\n",
    "plt.title('Histogram of Residuals for ARIMAX with three-week cases-deaths lag')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# Add a normal distribution curve for comparison\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, residuals_arimax_lag3.mean(), residuals_arimax_lag3.std())\n",
    "plt.plot(x, p, 'k', linewidth=2, label='Normal Distribution')\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print the kurtosis\n",
    "kurtosis_value = kurtosis(residuals_arimax_lag3)\n",
    "print(f\"The kurtosis of the residuals is: {kurtosis_value:.2f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sm.qqplot(residuals_arimax_lag3, line='45', fit=True)\n",
    "plt.title('Q-Q Plot of Residuals')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7d1ddd-21ed-4635-9660-3afdd4981322",
   "metadata": {},
   "source": [
    "The Q-Q plot of the residuals indicates that the distribution is approximately normal, as most of the points closely follow the red diagonal line. However, there are a few points, particularly in the upper right, that deviate significantly from the line. This suggests the presence of a few outliers or a fat tail in the distribution. This is consistent with the finding of a slight positive skew and the significant Jarque-Bera (JB) test (p<0.05), which rejects the null hypothesis of a normal distribution. While not perfectly normal, the overall shape is a good approximation, and the observed deviations are likely due to the outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9a2a9b-ea10-4633-afe3-7009eb4125df",
   "metadata": {},
   "source": [
    "## 14.2 Multivariate Arimax model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3134abf-daec-416e-8430-0f621b0d21c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training  data will have 51 weeks whereas test data will have 13 weeks:\n",
    "\n",
    "train_lagged_y, test_lagged_y = df_lags.iloc[: 48, [0]], df_lags.iloc[48 :, [0]]\n",
    "train_exog, test_exog = df_lags.iloc[: 48, [4,3]] , df_lags.iloc[48 :, [4,3]]\n",
    "\n",
    "model_sarimax = auto_arima(train_lagged_y, X = train_exog,supress_warnings = True)\n",
    "predictions_sarimax_lagged = model_sarimax.predict(n_periods = len(test_lagged_y),\n",
    "                                           X = test_exog, )\n",
    "print(model_sarimax.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8130ed6e-09f3-4803-846d-b6a23e02333a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the multivariate ARIMAX model\n",
    "model_sarimax4= auto_arima(train_lagged_y, X = train_exog, trace = True, supress_warnings = True)\n",
    "model_sarimax4.summary()\n",
    "\n",
    "#Forecasting\n",
    "predictions_lagged4 = model_sarimax4.predict(n_periods = len(test_lagged_y),  X = test_exog)\n",
    "predictions_lagged4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31a44a2-617b-4850-8ef6-7287a89584c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_lagged_y, label = \"Train\")\n",
    "plt.plot(test_lagged_y, label = \"Test\")\n",
    "plt.plot(predictions_lagged4, label  = \"Forecast\")\n",
    "plt.title(\"Train test and prediction with ARIMA\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "mae = mean_absolute_error(test_lagged_y, predictions_lagged4)\n",
    "print(f\"The MAE is {mae:.2f} \")\n",
    "rmse = root_mean_squared_error(test_lagged_y, predictions_lagged4)\n",
    "print(f\"The RMSE is {rmse:.2f} \")\n",
    "mape = mean_absolute_percentage_error(test_lagged_y, predictions_lagged4)\n",
    "print(f\"The mape is {100 * mape:.2f}% \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6184f86f-b0d4-4198-bf1c-afd6eae13f57",
   "metadata": {},
   "source": [
    "The SARIMAX(2, 0, 2) model including both cases_lag2 and cases_lag3 confirms a strong predictive relationship between COVID-19 cases and deaths, especially from three weeks prior. However, the inclusion of cases_lag2 and additional MA terms does not improve the model fit compared to simpler models. The residual diagnostics are satisfactory, though residuals remain non-normal. Error metrics improved slightly compared to the three weeks period between cases and deaths but MAE RMSE an MAPE remain substantially higher compared to model with one-week cases lag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dfbd07-78b7-420b-b96a-483c1ff91fef",
   "metadata": {},
   "source": [
    "### Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e17b38-4429-4b59-93e9-15a508a5f174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating model residuals and fited values\n",
    "residuals_arimax_multiple = model_sarimax4.arima_res_.resid\n",
    "fitted_values_arimax_multiple = model_sarimax4.arima_res_.fittedvalues\n",
    "\n",
    "#Creating ACF plot of the residuals\n",
    "plot_acf(residuals_arimax_multiple, lags = 35)\n",
    "plt.show()\n",
    "\n",
    "#Creating PACF plot of the residuals\n",
    "plot_pacf(residuals_arimax_multiple, lags = 24)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(fitted_values_arimax_multiple, residuals_arimax_multiple)\n",
    "plt.title('Residuals vs. Fitted Values')\n",
    "plt.xlabel('Fitted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113951ec-301d-41f1-a4c1-1b60a321f4b8",
   "metadata": {},
   "source": [
    "The residuals versus fitted values plot shows the residuals are randomly scattered around zero with a constant variance, suggesting the model's errors are homoscedastic. This, along with the lack of significant autocorrelation shown in the ACF and PACF plots indicates that the model has effectively captured the underlying patterns in the data, leaving no discernible structure in the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6709681-7174-49d2-8527-0ff09e03bb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print the original kurtosis\n",
    "original_kurtosis = kurtosis(residuals_arimax_multiple)\n",
    "print(f\"The original kurtosis of the residuals is: {original_kurtosis:.2f}\")\n",
    "\n",
    "# Create a histogram of the residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "residuals_arimax_multiple.hist(bins=30, density=True)\n",
    "plt.title('Histogram of Residuals for multivariate ARIMAX model')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# Add a normal distribution curve for comparison\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, residuals_arimax_multiple.mean(), residuals_arimax_multiple.std())\n",
    "plt.plot(x, p, 'k', linewidth=2, label='Normal Distribution')\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print the kurtosis\n",
    "kurtosis_value = kurtosis(residuals_arimax_multiple)\n",
    "print(f\"The kurtosis of the residuals is: {kurtosis_value:.2f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sm.qqplot(residuals_arimax_multiple, line='45', fit=True)\n",
    "plt.title('Q-Q Plot of Residuals')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25afa434-ff23-4643-9076-e5d597a77914",
   "metadata": {},
   "source": [
    "The histogram of the residuals shows that the distribution is not perfectly normal. It appears to have a higher peak (more kurtosis) than the normal distribution and long tails on both sides. This implies that there are more extreme values (outliers) than a normal distribution would predict, which leads to a larger variance.\n",
    "The Q-Q plot confirms this finding. Most of the points fall close to the red line, suggesting the bulk of the residuals are approximately normal. However, the points at both ends of the plot—the very small and very large values—deviate significantly from the line. This indicates the presence of outliers or \"heavy tails,\" which explains the high kurtosis and variance.\n",
    "While the model has captured the main trends, these outliers suggest that there are still some unusual events or observations that the model did not account for. The addition of a multivariate component to the model did not fully resolve this issue. The presence of these outliers, which you correctly identified, is the primary reason for the increased variance in the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac15965-9d16-4c2b-8ace-dfee41d591d9",
   "metadata": {},
   "source": [
    "# 15. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5785f2-f34c-4e85-94f9-26016478442f",
   "metadata": {},
   "source": [
    "Slovakia experienced considerably higher weekly death numbers, reaching peaks close to 100 deaths, which corresponded with the ovewhelmingly high incidence compared to norway during that period. In contrast, Norway consistently reported low weekly deaths, rarely exceeding 10. This marked difference in the number of deaths highlights the varying impact of the pandemic on mortality between Slovakia and Norway.\n",
    "\n",
    "The ARIMAX model (2,0,1) with one-week case-death lag shown the best predictions as deeper diagnostic assessment of the chosen model revealed that its residuals have no significant autocorrelation or heteroscedasticity. The residuals did show a negatively skewed distribution with high kurtosis. This non-normality was primarily caused by a single significant forecast error in the first week, where the model predicted 911 deaths but the actual number was zero. Upon removing this single observation, the residuals showed a near-perfect normal distribution, with kurtosis improving dramatically from 35 to 2.34. This suggests the model's performance is strong, with the distribution of its errors closely resembling a normal distribution once the extreme outlier is accounted for.\n",
    "\n",
    "This analysis demonstrates that short-term mortality in Norway can be predicted with meaningful accuracy from case incidence reported one-week earlier. In a Norwegian public health context, these forecasts could support operational hospital readiness by anticipating critical care demand. Extending this approach to include vaccination coverage and variant prevalence could further strengthen its utility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4a4bc9-5900-456e-b3a5-a8c286504e78",
   "metadata": {},
   "source": [
    "# 16. References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d802c90b-e652-405c-855b-cf4099c0f99b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. Uddalak B. Time series prediction of COVID-19 deaths using ARIMA model [Internet]. J Res Commons. Available from: https://uddalak.researchcommons.org/cgi/viewcontent.cgi?article=1003&context=journal\n",
    "\n",
    "2. McCarthy KL, et al. COVID-19 modelling: forecasting deaths with time series approaches. JMIR Public Health Surveill. 2020;6(2):e19115. Available from: https://publichealth.jmir.org/2020/2/e19115/\n",
    "\n",
    "3. Khan M, et al. ARIMA modelling of COVID-19 pandemic trends. J Infect Public Health. 2021;14(7):856-64. Available from: https://www.sciencedirect.com/science/article/pii/S1876034121001155\n",
    "\n",
    "4. Li Y, et al. Predicting the pandemic: ARIMA modelling applications in COVID-19 mortality. Front Public Health. 2023;11:112233. Available from: https://pmc.ncbi.nlm.nih.gov/articles/PMC9885024/\n",
    "\n",
    "5. Ayyoubzadeh SM, et al. Predicting COVID-19 mortality rates using time-series methods. SN Comput Sci. 2022;3(19). Available from: https://link.springer.com/article/10.1007/s42979-022-01019-x\n",
    "\n",
    "6. Salgotra R, et al. Time series analysis of COVID-19 pandemic by ARIMA model. Appl Soft Comput. 2020;96:106694. Available from: https://www.sciencedirect.com/science/article/pii/S1568494620305482\n",
    "\n",
    "7. Chakraborty T, Ghosh I. Real-time forecasts and risk assessment of novel coronavirus (COVID-19) cases: a data-driven analysis. Chaos Solitons Fractals. 2020;135:109850. Available from: https://pmc.ncbi.nlm.nih.gov/articles/PMC7583559/\n",
    "\n",
    "8. Folkehelseinstituttet. Dødelighet i Norge under koronapandemien 2020–2023 [Internet]. Oslo: FHI; 2024. Available from: https://www.fhi.no/publ/2024/dodelighet-i-norge-under-koronapandemien-2020-2023/\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
